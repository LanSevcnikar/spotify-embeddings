{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete all variables in the current scope\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cuda cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "max_files = 200\n",
    "path_to_files = '../embeddings/data/raw_data/'\n",
    "\n",
    "min_len_playlist = 17\n",
    "min_occurrences = 5\n",
    "\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = min_len_playlist - 1\n",
    "max_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 4e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "\n",
    "n_embd = 640\n",
    "n_head = 20\n",
    "n_layer = 4\n",
    "\n",
    "dropout = 0.05\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:55<00:00,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of playlists: 200000\n",
      "Number of tracks: 13307825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = []\n",
    "for i, file in tqdm(enumerate(os.listdir(path_to_files)[:max_files]), total=max_files):\n",
    "    if i == max_files:\n",
    "        break\n",
    "    with open(path_to_files + file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        playlists = data['playlists']\n",
    "        for playlist in playlists:\n",
    "            cleaned_playlist = []\n",
    "            for track in playlist['tracks']:\n",
    "                cleaned_playlist.append(track['track_uri'])\n",
    "            corpus.append(cleaned_playlist)\n",
    "\n",
    "print(f'Number of playlists: {len(corpus)}')\n",
    "print(f'Number of tracks: {sum([len(playlist) for playlist in corpus])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of playlists: 168168\n",
      "Number of playlists: 167702\n",
      "Number of playlists: 167646\n",
      "Number of playlists: 167633\n",
      "Number of playlists: 167629\n",
      "Number of playlists: 167626\n",
      "Number of playlists: 167626\n",
      "Number of playlists: 167626\n",
      "\n",
      "['spotify:track:0UaMYEvWZi0ZqiDOoHU3YI', 'spotify:track:6I9VzXrHxO9rA9A5euc8Ak', 'spotify:track:0WqIKmW4BTrj3eJFmnCKMv', 'spotify:track:1AWQoqb9bSvzTjaLralEkT', 'spotify:track:1lzr43nnXAijIGYnCT8M8H', 'spotify:track:0XUfyU2QviPAs6bxSpXYG4', 'spotify:track:68vgtRHr7iZHpzGpon6Jlo', 'spotify:track:3BxWKCI06eQ5Od8TY2JBeA', 'spotify:track:7H6ev70Weq6DdpZyyTmUXk', 'spotify:track:2PpruBYCo4H7WOBJ7Q2EwM', 'spotify:track:2gam98EZKrF9XuOkU13ApN', 'spotify:track:4Y45aqo9QMa57rDsAJv40A', 'spotify:track:1HwpWwa6bnqqRhK8agG4RS', 'spotify:track:20ORwCJusz4KS2PbTPVNKo', 'spotify:track:7k6IzwMGpxnRghE7YosnXT', 'spotify:track:1Bv0Yl01xBDZD4OQP93fyl', 'spotify:track:4omisSlTk6Dsq2iQD7MA07', 'spotify:track:7xYnUQigPoIDAMPVK79NEq', 'spotify:track:6d8A5sAx9TfdeseDvfWNHd', 'spotify:track:4pmc2AxSEq6g7hPVlJCPyP', 'spotify:track:215JYyyUnrJ98NK3KEwu6d', 'spotify:track:0uqPG793dkDDN7sCUJJIVC', 'spotify:track:19Js5ypV6JKn4DMExHQbGc', 'spotify:track:1JURww012QnWAw0zZXi6Aa', 'spotify:track:7DFnq8FYhHMCylykf6ZCxA', 'spotify:track:1TfAhjzRBWzYZ8IdUV3igl', 'spotify:track:1Y4ZdPOOgCUhBcKZOrUFiS', 'spotify:track:6MjljecHzHelUDismyKkba', 'spotify:track:67T6l4q3zVjC5nZZPXByU8', 'spotify:track:34ceTg8ChN5HjrqiIYCn9Q', 'spotify:track:5Q0Nhxo0l2bP3pNjpGJwV1', 'spotify:track:6GIrIt2M39wEGwjCQjGChX', 'spotify:track:4E5P1XyAFtrjpiIxkydly4', 'spotify:track:3H1LCvO3fVsK2HPguhbml0', 'spotify:track:3uoQULcUWfnt6nc6J7Vgai', 'spotify:track:2nbClS09zsIAqNkshg6jnp', 'spotify:track:69ghzc538EQSVon2Gm3wrr', 'spotify:track:1kusepF3AacIEtUTYrw4GV', 'spotify:track:7oK9VyNzrYvRFo7nQEYkWN', 'spotify:track:12qZHAeOyTf93YAWvGDTat', 'spotify:track:2jFlMILIQzs7lSFudG9lbo', 'spotify:track:4I2GqMe7L2ccMpUbnDzYLH', 'spotify:track:5lDriBxJd22IhOH9zTcFrV', 'spotify:track:2eJ8ij1T3cNUKiGdcUvKhy', 'spotify:track:5y69gQtK33qxb8a24ACkCy', 'spotify:track:1X5WGCrUMuwRFuYU1eAo2I', 'spotify:track:3utIAb67sOu0QHxBE88P1M', 'spotify:track:3jkdQNkDTxxXtjSO4l0o1H', 'spotify:track:5c1sfI6wIQEsSUw0xrkFdl', 'spotify:track:6sqNctd7MlJoKDOxPVCAvU', 'spotify:track:1b7vg5T9YKR3NNqXfBYRF7', 'spotify:track:6GIrIt2M39wEGwjCQjGChX']\n"
     ]
    }
   ],
   "source": [
    "def filter_corpus(corpus, min_len_playlist, min_occurrences):\n",
    "    song_counts = {}\n",
    "    for playlist in corpus:\n",
    "        for song in playlist:\n",
    "            if song in song_counts:\n",
    "                song_counts[song] += 1\n",
    "            else:\n",
    "                song_counts[song] = 1\n",
    "    song_counts = {k: v for k, v in song_counts.items() if v >= min_occurrences}\n",
    "\n",
    "    for i, playlist in enumerate(corpus):\n",
    "        corpus[i] = [song for song in playlist if song in song_counts]\n",
    "    \n",
    "    filtered_corpus = []\n",
    "    for playlist in corpus:\n",
    "        if len(playlist) >= min_len_playlist:\n",
    "            filtered_corpus.append(playlist)\n",
    "\n",
    "    return filtered_corpus\n",
    "\n",
    "total_len = sum([len(playlist) for playlist in corpus])\n",
    "corpus = filter_corpus(corpus, min_len_playlist, min_occurrences)\n",
    "print(f'Number of playlists: {len(corpus)}')\n",
    "\n",
    "while sum([len(playlist) for playlist in corpus]) != total_len:\n",
    "    total_len = sum([len(playlist) for playlist in corpus])\n",
    "    corpus = filter_corpus(corpus, min_len_playlist, min_occurrences)\n",
    "    print(f'Number of playlists: {len(corpus)}')\n",
    "\n",
    "print()\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206091\n",
      "['spotify:track:000VZqvXwT0YNqKk7iG2GS', 'spotify:track:000x2qE0ZI3hodeVrnJK8A', 'spotify:track:000xQL6tZNLJzIrtIgxqSl']\n",
      "['spotify:track:7zzLt6Z9y7jMvXnEg00n58', 'spotify:track:7zzbfi8fvHe6hm342GcNYl', 'spotify:track:7zzmpRP0WkYge45l6LTQ8i']\n"
     ]
    }
   ],
   "source": [
    "all_songs = []\n",
    "for playlist in corpus:\n",
    "    all_songs.extend(playlist)\n",
    "all_songs = list(set(all_songs))\n",
    "all_songs = sorted(all_songs)\n",
    "\n",
    "print(len(all_songs))\n",
    "print(all_songs[:3])\n",
    "print(all_songs[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13172, 166465, 14162, 31230, 47121, 14465, 162522, 84698, 192544, 64125, 71336, 120472, 34348, 53291, 199325, 31814, 127611, 205018, 175352, 128074, 53591, 24401, 30719, 34980, 190842, 39285, 41187, 168417, 161915, 81586, 143503, 165704, 111952, 86877, 103710, 74376, 162851, 46640, 201116, 27931, 72502, 113629, 152581, 70373, 157946, 40757, 103745, 99020, 148579, 182240, 42462, 165704]\n"
     ]
    }
   ],
   "source": [
    "stoi = {song:i for i, song in enumerate(all_songs)}\n",
    "itos = {i:song for i, song in enumerate(all_songs)}\n",
    "\n",
    "encode = lambda playlist: [stoi[song] for song in playlist]\n",
    "decode = lambda array: [itos[song] for song in array]\n",
    "\n",
    "vocab_size = len(all_songs)\n",
    "\n",
    "print(encode(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 33, 60, 77, 17]\n"
     ]
    }
   ],
   "source": [
    "data = [torch.tensor(encode(playlist)) for playlist in corpus]\n",
    "for index in range(len(data)):\n",
    "    data[index] = data[index].to(device)\n",
    "print([len(playlist) for playlist in data][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150863\n"
     ]
    }
   ],
   "source": [
    "n = int(len(corpus) * 0.9)\n",
    "print(n)\n",
    "\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "train_probs = np.array([len(playlist) - block_size for playlist in train_data])\n",
    "train_probs = train_probs / train_probs.sum()\n",
    "\n",
    "val_probs = np.array([len(playlist) - block_size for playlist in val_data])\n",
    "val_probs = val_probs / val_probs.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    probs = train_probs if split == 'train' else val_probs\n",
    "\n",
    "    ix = np.random.choice(len(data), p=probs, size=batch_size)\n",
    "    ixs = [np.random.randint(0, len(data[i]) - block_size) for i in ix]\n",
    "\n",
    "    x = torch.stack([data[ix[i]][ixs[i]:ixs[i] + block_size] for i in range(batch_size)])\n",
    "    y = torch.stack([data[ix[i]][ixs[i] + 1:ixs[i] + block_size + 1] for i in range(batch_size)])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([64, 16])\n",
      "tensor([[  9786,  26730,  74019,  ..., 116364,  67205, 172018],\n",
      "        [ 19824, 160175, 175782,  ...,   7203,  68253,  34104],\n",
      "        [112641,  21906,  61262,  ...,  72810, 149679,  49290],\n",
      "        ...,\n",
      "        [167433, 111980,  66634,  ...,  29643, 138150, 117130],\n",
      "        [190170,  97698,  46761,  ...,  88701, 104556, 188503],\n",
      "        [ 47339, 151242, 143119,  ..., 132513,  33009, 121496]],\n",
      "       device='cuda:0')\n",
      "targets:\n",
      "torch.Size([64, 16])\n",
      "tensor([[ 26730,  74019, 166159,  ...,  67205, 172018, 145162],\n",
      "        [160175, 175782, 152788,  ...,  68253,  34104, 191773],\n",
      "        [ 21906,  61262, 114840,  ..., 149679,  49290,  15106],\n",
      "        ...,\n",
      "        [111980,  66634, 102659,  ..., 138150, 117130, 138145],\n",
      "        [ 97698,  46761,  69927,  ..., 104556, 188503,   4814],\n",
      "        [151242, 143119,  94113,  ...,  33009, 121496, 151033]],\n",
      "       device='cuda:0')\n",
      "----\n",
      "when input is [9786] the target: 26730\n",
      "when input is [9786, 26730] the target: 74019\n",
      "when input is [9786, 26730, 74019] the target: 166159\n",
      "when input is [9786, 26730, 74019, 166159] the target: 101966\n",
      "when input is [9786, 26730, 74019, 166159, 101966] the target: 64715\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715] the target: 116518\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518] the target: 45148\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148] the target: 13651\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651] the target: 173315\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651, 173315] the target: 137875\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651, 173315, 137875] the target: 134931\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651, 173315, 137875, 134931] the target: 35747\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651, 173315, 137875, 134931, 35747] the target: 116364\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651, 173315, 137875, 134931, 35747, 116364] the target: 67205\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651, 173315, 137875, 134931, 35747, 116364, 67205] the target: 172018\n",
      "when input is [9786, 26730, 74019, 166159, 101966, 64715, 116518, 45148, 13651, 173315, 137875, 134931, 35747, 116364, 67205, 172018] the target: 145162\n",
      "when input is [19824] the target: 160175\n",
      "when input is [19824, 160175] the target: 175782\n",
      "when input is [19824, 160175, 175782] the target: 152788\n",
      "when input is [19824, 160175, 175782, 152788] the target: 116082\n",
      "when input is [19824, 160175, 175782, 152788, 116082] the target: 92585\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585] the target: 138180\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180] the target: 64418\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418] the target: 24121\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121] the target: 96575\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121, 96575] the target: 158809\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121, 96575, 158809] the target: 177946\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121, 96575, 158809, 177946] the target: 158579\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121, 96575, 158809, 177946, 158579] the target: 7203\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121, 96575, 158809, 177946, 158579, 7203] the target: 68253\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121, 96575, 158809, 177946, 158579, 7203, 68253] the target: 34104\n",
      "when input is [19824, 160175, 175782, 152788, 116082, 92585, 138180, 64418, 24121, 96575, 158809, 177946, 158579, 7203, 68253, 34104] the target: 191773\n",
      "when input is [112641] the target: 21906\n",
      "when input is [112641, 21906] the target: 61262\n",
      "when input is [112641, 21906, 61262] the target: 114840\n",
      "when input is [112641, 21906, 61262, 114840] the target: 63596\n",
      "when input is [112641, 21906, 61262, 114840, 63596] the target: 12544\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544] the target: 74159\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159] the target: 175499\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499] the target: 120979\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979] the target: 70399\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979, 70399] the target: 12032\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979, 70399, 12032] the target: 174813\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979, 70399, 12032, 174813] the target: 97707\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979, 70399, 12032, 174813, 97707] the target: 72810\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979, 70399, 12032, 174813, 97707, 72810] the target: 149679\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979, 70399, 12032, 174813, 97707, 72810, 149679] the target: 49290\n",
      "when input is [112641, 21906, 61262, 114840, 63596, 12544, 74159, 175499, 120979, 70399, 12032, 174813, 97707, 72810, 149679, 49290] the target: 15106\n",
      "when input is [25678] the target: 16881\n",
      "when input is [25678, 16881] the target: 174498\n",
      "when input is [25678, 16881, 174498] the target: 2169\n",
      "when input is [25678, 16881, 174498, 2169] the target: 195605\n",
      "when input is [25678, 16881, 174498, 2169, 195605] the target: 2293\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293] the target: 63323\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323] the target: 165998\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998] the target: 192292\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292] the target: 44163\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292, 44163] the target: 171765\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292, 44163, 171765] the target: 124451\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292, 44163, 171765, 124451] the target: 39517\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292, 44163, 171765, 124451, 39517] the target: 22852\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292, 44163, 171765, 124451, 39517, 22852] the target: 16179\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292, 44163, 171765, 124451, 39517, 22852, 16179] the target: 144709\n",
      "when input is [25678, 16881, 174498, 2169, 195605, 2293, 63323, 165998, 192292, 44163, 171765, 124451, 39517, 22852, 16179, 144709] the target: 25684\n",
      "when input is [159932] the target: 192544\n",
      "when input is [159932, 192544] the target: 92009\n",
      "when input is [159932, 192544, 92009] the target: 82889\n",
      "when input is [159932, 192544, 92009, 82889] the target: 44457\n",
      "when input is [159932, 192544, 92009, 82889, 44457] the target: 186758\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758] the target: 86163\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163] the target: 115051\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051] the target: 178493\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493] the target: 38343\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493, 38343] the target: 76991\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493, 38343, 76991] the target: 59224\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493, 38343, 76991, 59224] the target: 154645\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493, 38343, 76991, 59224, 154645] the target: 48573\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493, 38343, 76991, 59224, 154645, 48573] the target: 146074\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493, 38343, 76991, 59224, 154645, 48573, 146074] the target: 23182\n",
      "when input is [159932, 192544, 92009, 82889, 44457, 186758, 86163, 115051, 178493, 38343, 76991, 59224, 154645, 48573, 146074, 23182] the target: 24539\n",
      "when input is [24033] the target: 174640\n",
      "when input is [24033, 174640] the target: 129424\n",
      "when input is [24033, 174640, 129424] the target: 177862\n",
      "when input is [24033, 174640, 129424, 177862] the target: 178825\n",
      "when input is [24033, 174640, 129424, 177862, 178825] the target: 159318\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318] the target: 161651\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651] the target: 155279\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279] the target: 68235\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235] the target: 142290\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235, 142290] the target: 164475\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235, 142290, 164475] the target: 201932\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235, 142290, 164475, 201932] the target: 9043\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235, 142290, 164475, 201932, 9043] the target: 16229\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235, 142290, 164475, 201932, 9043, 16229] the target: 174966\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235, 142290, 164475, 201932, 9043, 16229, 174966] the target: 160820\n",
      "when input is [24033, 174640, 129424, 177862, 178825, 159318, 161651, 155279, 68235, 142290, 164475, 201932, 9043, 16229, 174966, 160820] the target: 94497\n",
      "when input is [51349] the target: 31508\n",
      "when input is [51349, 31508] the target: 22263\n",
      "when input is [51349, 31508, 22263] the target: 177054\n",
      "when input is [51349, 31508, 22263, 177054] the target: 20390\n",
      "when input is [51349, 31508, 22263, 177054, 20390] the target: 62813\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813] the target: 119925\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925] the target: 140047\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047] the target: 34217\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217] the target: 23311\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217, 23311] the target: 120995\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217, 23311, 120995] the target: 65066\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217, 23311, 120995, 65066] the target: 9157\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217, 23311, 120995, 65066, 9157] the target: 162175\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217, 23311, 120995, 65066, 9157, 162175] the target: 127028\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217, 23311, 120995, 65066, 9157, 162175, 127028] the target: 204893\n",
      "when input is [51349, 31508, 22263, 177054, 20390, 62813, 119925, 140047, 34217, 23311, 120995, 65066, 9157, 162175, 127028, 204893] the target: 64425\n",
      "when input is [129676] the target: 161779\n",
      "when input is [129676, 161779] the target: 113477\n",
      "when input is [129676, 161779, 113477] the target: 43891\n",
      "when input is [129676, 161779, 113477, 43891] the target: 96633\n",
      "when input is [129676, 161779, 113477, 43891, 96633] the target: 133328\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328] the target: 92441\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441] the target: 12755\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755] the target: 143581\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581] the target: 27351\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581, 27351] the target: 146455\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581, 27351, 146455] the target: 127421\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581, 27351, 146455, 127421] the target: 135372\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581, 27351, 146455, 127421, 135372] the target: 27830\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581, 27351, 146455, 127421, 135372, 27830] the target: 117493\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581, 27351, 146455, 127421, 135372, 27830, 117493] the target: 21410\n",
      "when input is [129676, 161779, 113477, 43891, 96633, 133328, 92441, 12755, 143581, 27351, 146455, 127421, 135372, 27830, 117493, 21410] the target: 133584\n",
      "when input is [96940] the target: 62229\n",
      "when input is [96940, 62229] the target: 159218\n",
      "when input is [96940, 62229, 159218] the target: 97544\n",
      "when input is [96940, 62229, 159218, 97544] the target: 81967\n",
      "when input is [96940, 62229, 159218, 97544, 81967] the target: 114635\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635] the target: 192850\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850] the target: 12211\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211] the target: 173130\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130] the target: 63957\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130, 63957] the target: 157258\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130, 63957, 157258] the target: 104655\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130, 63957, 157258, 104655] the target: 35529\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130, 63957, 157258, 104655, 35529] the target: 52341\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130, 63957, 157258, 104655, 35529, 52341] the target: 181114\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130, 63957, 157258, 104655, 35529, 52341, 181114] the target: 66513\n",
      "when input is [96940, 62229, 159218, 97544, 81967, 114635, 192850, 12211, 173130, 63957, 157258, 104655, 35529, 52341, 181114, 66513] the target: 31163\n",
      "when input is [128500] the target: 86163\n",
      "when input is [128500, 86163] the target: 27347\n",
      "when input is [128500, 86163, 27347] the target: 17904\n",
      "when input is [128500, 86163, 27347, 17904] the target: 204828\n",
      "when input is [128500, 86163, 27347, 17904, 204828] the target: 76868\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868] the target: 110787\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787] the target: 123202\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202] the target: 31099\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099] the target: 159128\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099, 159128] the target: 40832\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099, 159128, 40832] the target: 138325\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099, 159128, 40832, 138325] the target: 45960\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099, 159128, 40832, 138325, 45960] the target: 50905\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099, 159128, 40832, 138325, 45960, 50905] the target: 84619\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099, 159128, 40832, 138325, 45960, 50905, 84619] the target: 199679\n",
      "when input is [128500, 86163, 27347, 17904, 204828, 76868, 110787, 123202, 31099, 159128, 40832, 138325, 45960, 50905, 84619, 199679] the target: 120952\n",
      "when input is [29366] the target: 117780\n",
      "when input is [29366, 117780] the target: 165875\n",
      "when input is [29366, 117780, 165875] the target: 164560\n",
      "when input is [29366, 117780, 165875, 164560] the target: 11145\n",
      "when input is [29366, 117780, 165875, 164560, 11145] the target: 140727\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727] the target: 180446\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446] the target: 82697\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697] the target: 71852\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852] the target: 135087\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852, 135087] the target: 179402\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852, 135087, 179402] the target: 182691\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852, 135087, 179402, 182691] the target: 139348\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852, 135087, 179402, 182691, 139348] the target: 135087\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852, 135087, 179402, 182691, 139348, 135087] the target: 65772\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852, 135087, 179402, 182691, 139348, 135087, 65772] the target: 57326\n",
      "when input is [29366, 117780, 165875, 164560, 11145, 140727, 180446, 82697, 71852, 135087, 179402, 182691, 139348, 135087, 65772, 57326] the target: 108407\n",
      "when input is [71146] the target: 201379\n",
      "when input is [71146, 201379] the target: 101092\n",
      "when input is [71146, 201379, 101092] the target: 191601\n",
      "when input is [71146, 201379, 101092, 191601] the target: 27983\n",
      "when input is [71146, 201379, 101092, 191601, 27983] the target: 143121\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121] the target: 58515\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515] the target: 14954\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954] the target: 70399\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399] the target: 70399\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399, 70399] the target: 11911\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399, 70399, 11911] the target: 11911\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399, 70399, 11911, 11911] the target: 31782\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399, 70399, 11911, 11911, 31782] the target: 31782\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399, 70399, 11911, 11911, 31782, 31782] the target: 133305\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399, 70399, 11911, 11911, 31782, 31782, 133305] the target: 133305\n",
      "when input is [71146, 201379, 101092, 191601, 27983, 143121, 58515, 14954, 70399, 70399, 11911, 11911, 31782, 31782, 133305, 133305] the target: 97062\n",
      "when input is [96125] the target: 181476\n",
      "when input is [96125, 181476] the target: 28374\n",
      "when input is [96125, 181476, 28374] the target: 107602\n",
      "when input is [96125, 181476, 28374, 107602] the target: 17528\n",
      "when input is [96125, 181476, 28374, 107602, 17528] the target: 178334\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334] the target: 10596\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596] the target: 73060\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060] the target: 39015\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015] the target: 107552\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015, 107552] the target: 5032\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015, 107552, 5032] the target: 140346\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015, 107552, 5032, 140346] the target: 10701\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015, 107552, 5032, 140346, 10701] the target: 69648\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015, 107552, 5032, 140346, 10701, 69648] the target: 191710\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015, 107552, 5032, 140346, 10701, 69648, 191710] the target: 52101\n",
      "when input is [96125, 181476, 28374, 107602, 17528, 178334, 10596, 73060, 39015, 107552, 5032, 140346, 10701, 69648, 191710, 52101] the target: 35074\n",
      "when input is [126308] the target: 59060\n",
      "when input is [126308, 59060] the target: 182755\n",
      "when input is [126308, 59060, 182755] the target: 108074\n",
      "when input is [126308, 59060, 182755, 108074] the target: 181464\n",
      "when input is [126308, 59060, 182755, 108074, 181464] the target: 71141\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141] the target: 99652\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652] the target: 160513\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513] the target: 181331\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331] the target: 165309\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331, 165309] the target: 99013\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331, 165309, 99013] the target: 191674\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331, 165309, 99013, 191674] the target: 65614\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331, 165309, 99013, 191674, 65614] the target: 85821\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331, 165309, 99013, 191674, 65614, 85821] the target: 34034\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331, 165309, 99013, 191674, 65614, 85821, 34034] the target: 107035\n",
      "when input is [126308, 59060, 182755, 108074, 181464, 71141, 99652, 160513, 181331, 165309, 99013, 191674, 65614, 85821, 34034, 107035] the target: 148951\n",
      "when input is [150434] the target: 143664\n",
      "when input is [150434, 143664] the target: 16521\n",
      "when input is [150434, 143664, 16521] the target: 2529\n",
      "when input is [150434, 143664, 16521, 2529] the target: 196522\n",
      "when input is [150434, 143664, 16521, 2529, 196522] the target: 91320\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320] the target: 44112\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112] the target: 44977\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977] the target: 53754\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754] the target: 67037\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754, 67037] the target: 21230\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754, 67037, 21230] the target: 68117\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754, 67037, 21230, 68117] the target: 106199\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754, 67037, 21230, 68117, 106199] the target: 199057\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754, 67037, 21230, 68117, 106199, 199057] the target: 95961\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754, 67037, 21230, 68117, 106199, 199057, 95961] the target: 48380\n",
      "when input is [150434, 143664, 16521, 2529, 196522, 91320, 44112, 44977, 53754, 67037, 21230, 68117, 106199, 199057, 95961, 48380] the target: 5627\n",
      "when input is [64125] the target: 94548\n",
      "when input is [64125, 94548] the target: 70367\n",
      "when input is [64125, 94548, 70367] the target: 203379\n",
      "when input is [64125, 94548, 70367, 203379] the target: 146806\n",
      "when input is [64125, 94548, 70367, 203379, 146806] the target: 100697\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697] the target: 9273\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273] the target: 80873\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873] the target: 190329\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329] the target: 39134\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329, 39134] the target: 176410\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329, 39134, 176410] the target: 204577\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329, 39134, 176410, 204577] the target: 188427\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329, 39134, 176410, 204577, 188427] the target: 70963\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329, 39134, 176410, 204577, 188427, 70963] the target: 173766\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329, 39134, 176410, 204577, 188427, 70963, 173766] the target: 22581\n",
      "when input is [64125, 94548, 70367, 203379, 146806, 100697, 9273, 80873, 190329, 39134, 176410, 204577, 188427, 70963, 173766, 22581] the target: 68052\n",
      "when input is [129789] the target: 199856\n",
      "when input is [129789, 199856] the target: 118632\n",
      "when input is [129789, 199856, 118632] the target: 128355\n",
      "when input is [129789, 199856, 118632, 128355] the target: 29643\n",
      "when input is [129789, 199856, 118632, 128355, 29643] the target: 159995\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995] the target: 79723\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723] the target: 34104\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104] the target: 124166\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166] the target: 681\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166, 681] the target: 89103\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166, 681, 89103] the target: 163101\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166, 681, 89103, 163101] the target: 166448\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166, 681, 89103, 163101, 166448] the target: 79222\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166, 681, 89103, 163101, 166448, 79222] the target: 7815\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166, 681, 89103, 163101, 166448, 79222, 7815] the target: 101566\n",
      "when input is [129789, 199856, 118632, 128355, 29643, 159995, 79723, 34104, 124166, 681, 89103, 163101, 166448, 79222, 7815, 101566] the target: 24425\n",
      "when input is [71054] the target: 84397\n",
      "when input is [71054, 84397] the target: 10115\n",
      "when input is [71054, 84397, 10115] the target: 100510\n",
      "when input is [71054, 84397, 10115, 100510] the target: 156650\n",
      "when input is [71054, 84397, 10115, 100510, 156650] the target: 69927\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927] the target: 145681\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681] the target: 80620\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620] the target: 62707\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707] the target: 173885\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707, 173885] the target: 95825\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707, 173885, 95825] the target: 120494\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707, 173885, 95825, 120494] the target: 101685\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707, 173885, 95825, 120494, 101685] the target: 161335\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707, 173885, 95825, 120494, 101685, 161335] the target: 113578\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707, 173885, 95825, 120494, 101685, 161335, 113578] the target: 128206\n",
      "when input is [71054, 84397, 10115, 100510, 156650, 69927, 145681, 80620, 62707, 173885, 95825, 120494, 101685, 161335, 113578, 128206] the target: 115733\n",
      "when input is [185314] the target: 60711\n",
      "when input is [185314, 60711] the target: 97431\n",
      "when input is [185314, 60711, 97431] the target: 67570\n",
      "when input is [185314, 60711, 97431, 67570] the target: 90543\n",
      "when input is [185314, 60711, 97431, 67570, 90543] the target: 189694\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694] the target: 42872\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872] the target: 40915\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915] the target: 204444\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444] the target: 13237\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444, 13237] the target: 70312\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444, 13237, 70312] the target: 74359\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444, 13237, 70312, 74359] the target: 52683\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444, 13237, 70312, 74359, 52683] the target: 40424\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444, 13237, 70312, 74359, 52683, 40424] the target: 129023\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444, 13237, 70312, 74359, 52683, 40424, 129023] the target: 100808\n",
      "when input is [185314, 60711, 97431, 67570, 90543, 189694, 42872, 40915, 204444, 13237, 70312, 74359, 52683, 40424, 129023, 100808] the target: 107469\n",
      "when input is [143377] the target: 124416\n",
      "when input is [143377, 124416] the target: 90629\n",
      "when input is [143377, 124416, 90629] the target: 177444\n",
      "when input is [143377, 124416, 90629, 177444] the target: 19907\n",
      "when input is [143377, 124416, 90629, 177444, 19907] the target: 50705\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705] the target: 148331\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331] the target: 93320\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320] the target: 118989\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989] the target: 123301\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989, 123301] the target: 43760\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989, 123301, 43760] the target: 87768\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989, 123301, 43760, 87768] the target: 203989\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989, 123301, 43760, 87768, 203989] the target: 48111\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989, 123301, 43760, 87768, 203989, 48111] the target: 65537\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989, 123301, 43760, 87768, 203989, 48111, 65537] the target: 5257\n",
      "when input is [143377, 124416, 90629, 177444, 19907, 50705, 148331, 93320, 118989, 123301, 43760, 87768, 203989, 48111, 65537, 5257] the target: 163391\n",
      "when input is [82374] the target: 156382\n",
      "when input is [82374, 156382] the target: 141357\n",
      "when input is [82374, 156382, 141357] the target: 74274\n",
      "when input is [82374, 156382, 141357, 74274] the target: 100478\n",
      "when input is [82374, 156382, 141357, 74274, 100478] the target: 40387\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387] the target: 121958\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958] the target: 107006\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006] the target: 104682\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682] the target: 16206\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682, 16206] the target: 148773\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682, 16206, 148773] the target: 112011\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682, 16206, 148773, 112011] the target: 201868\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682, 16206, 148773, 112011, 201868] the target: 761\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682, 16206, 148773, 112011, 201868, 761] the target: 117473\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682, 16206, 148773, 112011, 201868, 761, 117473] the target: 95291\n",
      "when input is [82374, 156382, 141357, 74274, 100478, 40387, 121958, 107006, 104682, 16206, 148773, 112011, 201868, 761, 117473, 95291] the target: 81640\n",
      "when input is [132892] the target: 205346\n",
      "when input is [132892, 205346] the target: 106867\n",
      "when input is [132892, 205346, 106867] the target: 200798\n",
      "when input is [132892, 205346, 106867, 200798] the target: 83470\n",
      "when input is [132892, 205346, 106867, 200798, 83470] the target: 154187\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187] the target: 160817\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817] the target: 190294\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294] the target: 146806\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806] the target: 125012\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806, 125012] the target: 154476\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806, 125012, 154476] the target: 129071\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806, 125012, 154476, 129071] the target: 23861\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806, 125012, 154476, 129071, 23861] the target: 1077\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806, 125012, 154476, 129071, 23861, 1077] the target: 32285\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806, 125012, 154476, 129071, 23861, 1077, 32285] the target: 191960\n",
      "when input is [132892, 205346, 106867, 200798, 83470, 154187, 160817, 190294, 146806, 125012, 154476, 129071, 23861, 1077, 32285, 191960] the target: 42114\n",
      "when input is [5329] the target: 5329\n",
      "when input is [5329, 5329] the target: 130469\n",
      "when input is [5329, 5329, 130469] the target: 74554\n",
      "when input is [5329, 5329, 130469, 74554] the target: 176299\n",
      "when input is [5329, 5329, 130469, 74554, 176299] the target: 106921\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921] the target: 129637\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637] the target: 129637\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637] the target: 157739\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739] the target: 100513\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739, 100513] the target: 163497\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739, 100513, 163497] the target: 203149\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739, 100513, 163497, 203149] the target: 101536\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739, 100513, 163497, 203149, 101536] the target: 80941\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739, 100513, 163497, 203149, 101536, 80941] the target: 16658\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739, 100513, 163497, 203149, 101536, 80941, 16658] the target: 188629\n",
      "when input is [5329, 5329, 130469, 74554, 176299, 106921, 129637, 129637, 157739, 100513, 163497, 203149, 101536, 80941, 16658, 188629] the target: 69445\n",
      "when input is [123937] the target: 69418\n",
      "when input is [123937, 69418] the target: 133709\n",
      "when input is [123937, 69418, 133709] the target: 186592\n",
      "when input is [123937, 69418, 133709, 186592] the target: 159314\n",
      "when input is [123937, 69418, 133709, 186592, 159314] the target: 191702\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702] the target: 68518\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518] the target: 59056\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056] the target: 25361\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361] the target: 97668\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361, 97668] the target: 153260\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361, 97668, 153260] the target: 48111\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361, 97668, 153260, 48111] the target: 151336\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361, 97668, 153260, 48111, 151336] the target: 138894\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361, 97668, 153260, 48111, 151336, 138894] the target: 38368\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361, 97668, 153260, 48111, 151336, 138894, 38368] the target: 116068\n",
      "when input is [123937, 69418, 133709, 186592, 159314, 191702, 68518, 59056, 25361, 97668, 153260, 48111, 151336, 138894, 38368, 116068] the target: 137907\n",
      "when input is [41133] the target: 96207\n",
      "when input is [41133, 96207] the target: 172523\n",
      "when input is [41133, 96207, 172523] the target: 166314\n",
      "when input is [41133, 96207, 172523, 166314] the target: 25163\n",
      "when input is [41133, 96207, 172523, 166314, 25163] the target: 74567\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567] the target: 197381\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381] the target: 31265\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265] the target: 135117\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117] the target: 29767\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117, 29767] the target: 92411\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117, 29767, 92411] the target: 37534\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117, 29767, 92411, 37534] the target: 186772\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117, 29767, 92411, 37534, 186772] the target: 78844\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117, 29767, 92411, 37534, 186772, 78844] the target: 82941\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117, 29767, 92411, 37534, 186772, 78844, 82941] the target: 119855\n",
      "when input is [41133, 96207, 172523, 166314, 25163, 74567, 197381, 31265, 135117, 29767, 92411, 37534, 186772, 78844, 82941, 119855] the target: 196056\n",
      "when input is [135495] the target: 141542\n",
      "when input is [135495, 141542] the target: 67712\n",
      "when input is [135495, 141542, 67712] the target: 56562\n",
      "when input is [135495, 141542, 67712, 56562] the target: 195485\n",
      "when input is [135495, 141542, 67712, 56562, 195485] the target: 101222\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222] the target: 41784\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784] the target: 53103\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103] the target: 105220\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220] the target: 179654\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220, 179654] the target: 138981\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220, 179654, 138981] the target: 40976\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220, 179654, 138981, 40976] the target: 150047\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220, 179654, 138981, 40976, 150047] the target: 59212\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220, 179654, 138981, 40976, 150047, 59212] the target: 121919\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220, 179654, 138981, 40976, 150047, 59212, 121919] the target: 154394\n",
      "when input is [135495, 141542, 67712, 56562, 195485, 101222, 41784, 53103, 105220, 179654, 138981, 40976, 150047, 59212, 121919, 154394] the target: 3234\n",
      "when input is [36294] the target: 108827\n",
      "when input is [36294, 108827] the target: 132323\n",
      "when input is [36294, 108827, 132323] the target: 8301\n",
      "when input is [36294, 108827, 132323, 8301] the target: 65706\n",
      "when input is [36294, 108827, 132323, 8301, 65706] the target: 166109\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109] the target: 151280\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280] the target: 18207\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207] the target: 167070\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070] the target: 116818\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070, 116818] the target: 14546\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070, 116818, 14546] the target: 46062\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070, 116818, 14546, 46062] the target: 124399\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070, 116818, 14546, 46062, 124399] the target: 120613\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070, 116818, 14546, 46062, 124399, 120613] the target: 192419\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070, 116818, 14546, 46062, 124399, 120613, 192419] the target: 52409\n",
      "when input is [36294, 108827, 132323, 8301, 65706, 166109, 151280, 18207, 167070, 116818, 14546, 46062, 124399, 120613, 192419, 52409] the target: 62651\n",
      "when input is [175509] the target: 33543\n",
      "when input is [175509, 33543] the target: 133186\n",
      "when input is [175509, 33543, 133186] the target: 2669\n",
      "when input is [175509, 33543, 133186, 2669] the target: 25744\n",
      "when input is [175509, 33543, 133186, 2669, 25744] the target: 141839\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839] the target: 36744\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744] the target: 158880\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880] the target: 24254\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254] the target: 106496\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254, 106496] the target: 142964\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254, 106496, 142964] the target: 180925\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254, 106496, 142964, 180925] the target: 90907\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254, 106496, 142964, 180925, 90907] the target: 44178\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254, 106496, 142964, 180925, 90907, 44178] the target: 49917\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254, 106496, 142964, 180925, 90907, 44178, 49917] the target: 15845\n",
      "when input is [175509, 33543, 133186, 2669, 25744, 141839, 36744, 158880, 24254, 106496, 142964, 180925, 90907, 44178, 49917, 15845] the target: 128173\n",
      "when input is [145453] the target: 89530\n",
      "when input is [145453, 89530] the target: 75454\n",
      "when input is [145453, 89530, 75454] the target: 10298\n",
      "when input is [145453, 89530, 75454, 10298] the target: 94350\n",
      "when input is [145453, 89530, 75454, 10298, 94350] the target: 5132\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132] the target: 203448\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448] the target: 146638\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638] the target: 169016\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016] the target: 147319\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016, 147319] the target: 176362\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016, 147319, 176362] the target: 84308\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016, 147319, 176362, 84308] the target: 90221\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016, 147319, 176362, 84308, 90221] the target: 177950\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016, 147319, 176362, 84308, 90221, 177950] the target: 100930\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016, 147319, 176362, 84308, 90221, 177950, 100930] the target: 136768\n",
      "when input is [145453, 89530, 75454, 10298, 94350, 5132, 203448, 146638, 169016, 147319, 176362, 84308, 90221, 177950, 100930, 136768] the target: 70560\n",
      "when input is [185111] the target: 135645\n",
      "when input is [185111, 135645] the target: 119790\n",
      "when input is [185111, 135645, 119790] the target: 154216\n",
      "when input is [185111, 135645, 119790, 154216] the target: 96530\n",
      "when input is [185111, 135645, 119790, 154216, 96530] the target: 160196\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196] the target: 97383\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383] the target: 151780\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780] the target: 25821\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821] the target: 56562\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821, 56562] the target: 197241\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821, 56562, 197241] the target: 140861\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821, 56562, 197241, 140861] the target: 12056\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821, 56562, 197241, 140861, 12056] the target: 103277\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821, 56562, 197241, 140861, 12056, 103277] the target: 142442\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821, 56562, 197241, 140861, 12056, 103277, 142442] the target: 10415\n",
      "when input is [185111, 135645, 119790, 154216, 96530, 160196, 97383, 151780, 25821, 56562, 197241, 140861, 12056, 103277, 142442, 10415] the target: 111218\n",
      "when input is [173717] the target: 168538\n",
      "when input is [173717, 168538] the target: 68094\n",
      "when input is [173717, 168538, 68094] the target: 86376\n",
      "when input is [173717, 168538, 68094, 86376] the target: 62943\n",
      "when input is [173717, 168538, 68094, 86376, 62943] the target: 136638\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638] the target: 126920\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920] the target: 118020\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020] the target: 82982\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982] the target: 26730\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982, 26730] the target: 17649\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982, 26730, 17649] the target: 149042\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982, 26730, 17649, 149042] the target: 154897\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982, 26730, 17649, 149042, 154897] the target: 108161\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982, 26730, 17649, 149042, 154897, 108161] the target: 136280\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982, 26730, 17649, 149042, 154897, 108161, 136280] the target: 78401\n",
      "when input is [173717, 168538, 68094, 86376, 62943, 136638, 126920, 118020, 82982, 26730, 17649, 149042, 154897, 108161, 136280, 78401] the target: 139144\n",
      "when input is [125838] the target: 144370\n",
      "when input is [125838, 144370] the target: 15376\n",
      "when input is [125838, 144370, 15376] the target: 97068\n",
      "when input is [125838, 144370, 15376, 97068] the target: 21542\n",
      "when input is [125838, 144370, 15376, 97068, 21542] the target: 54542\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542] the target: 17666\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666] the target: 30353\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353] the target: 31621\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621] the target: 175386\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621, 175386] the target: 17837\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621, 175386, 17837] the target: 87727\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621, 175386, 17837, 87727] the target: 164467\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621, 175386, 17837, 87727, 164467] the target: 155163\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621, 175386, 17837, 87727, 164467, 155163] the target: 153737\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621, 175386, 17837, 87727, 164467, 155163, 153737] the target: 5990\n",
      "when input is [125838, 144370, 15376, 97068, 21542, 54542, 17666, 30353, 31621, 175386, 17837, 87727, 164467, 155163, 153737, 5990] the target: 150698\n",
      "when input is [50818] the target: 61101\n",
      "when input is [50818, 61101] the target: 164599\n",
      "when input is [50818, 61101, 164599] the target: 47512\n",
      "when input is [50818, 61101, 164599, 47512] the target: 98264\n",
      "when input is [50818, 61101, 164599, 47512, 98264] the target: 30077\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077] the target: 8587\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587] the target: 88105\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105] the target: 173058\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058] the target: 69930\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058, 69930] the target: 143503\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058, 69930, 143503] the target: 75414\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058, 69930, 143503, 75414] the target: 196890\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058, 69930, 143503, 75414, 196890] the target: 158324\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058, 69930, 143503, 75414, 196890, 158324] the target: 24865\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058, 69930, 143503, 75414, 196890, 158324, 24865] the target: 42636\n",
      "when input is [50818, 61101, 164599, 47512, 98264, 30077, 8587, 88105, 173058, 69930, 143503, 75414, 196890, 158324, 24865, 42636] the target: 179857\n",
      "when input is [51627] the target: 165828\n",
      "when input is [51627, 165828] the target: 201961\n",
      "when input is [51627, 165828, 201961] the target: 171744\n",
      "when input is [51627, 165828, 201961, 171744] the target: 114089\n",
      "when input is [51627, 165828, 201961, 171744, 114089] the target: 96692\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692] the target: 94452\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452] the target: 31561\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561] the target: 65100\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100] the target: 81335\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100, 81335] the target: 148949\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100, 81335, 148949] the target: 50982\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100, 81335, 148949, 50982] the target: 195993\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100, 81335, 148949, 50982, 195993] the target: 64979\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100, 81335, 148949, 50982, 195993, 64979] the target: 200522\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100, 81335, 148949, 50982, 195993, 64979, 200522] the target: 179037\n",
      "when input is [51627, 165828, 201961, 171744, 114089, 96692, 94452, 31561, 65100, 81335, 148949, 50982, 195993, 64979, 200522, 179037] the target: 61222\n",
      "when input is [202077] the target: 140821\n",
      "when input is [202077, 140821] the target: 95431\n",
      "when input is [202077, 140821, 95431] the target: 42014\n",
      "when input is [202077, 140821, 95431, 42014] the target: 203080\n",
      "when input is [202077, 140821, 95431, 42014, 203080] the target: 19463\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463] the target: 32327\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327] the target: 1753\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753] the target: 178258\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258] the target: 52622\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258, 52622] the target: 70836\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258, 52622, 70836] the target: 134746\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258, 52622, 70836, 134746] the target: 109268\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258, 52622, 70836, 134746, 109268] the target: 36108\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258, 52622, 70836, 134746, 109268, 36108] the target: 133932\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258, 52622, 70836, 134746, 109268, 36108, 133932] the target: 192988\n",
      "when input is [202077, 140821, 95431, 42014, 203080, 19463, 32327, 1753, 178258, 52622, 70836, 134746, 109268, 36108, 133932, 192988] the target: 90325\n",
      "when input is [28170] the target: 64394\n",
      "when input is [28170, 64394] the target: 50125\n",
      "when input is [28170, 64394, 50125] the target: 25945\n",
      "when input is [28170, 64394, 50125, 25945] the target: 119566\n",
      "when input is [28170, 64394, 50125, 25945, 119566] the target: 38324\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324] the target: 104215\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215] the target: 7894\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894] the target: 51369\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369] the target: 116806\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369, 116806] the target: 15181\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369, 116806, 15181] the target: 106782\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369, 116806, 15181, 106782] the target: 152731\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369, 116806, 15181, 106782, 152731] the target: 169901\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369, 116806, 15181, 106782, 152731, 169901] the target: 97435\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369, 116806, 15181, 106782, 152731, 169901, 97435] the target: 11135\n",
      "when input is [28170, 64394, 50125, 25945, 119566, 38324, 104215, 7894, 51369, 116806, 15181, 106782, 152731, 169901, 97435, 11135] the target: 140924\n",
      "when input is [35515] the target: 201918\n",
      "when input is [35515, 201918] the target: 103483\n",
      "when input is [35515, 201918, 103483] the target: 20118\n",
      "when input is [35515, 201918, 103483, 20118] the target: 189771\n",
      "when input is [35515, 201918, 103483, 20118, 189771] the target: 154662\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662] the target: 20494\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494] the target: 154470\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470] the target: 157137\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137] the target: 25184\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137, 25184] the target: 136820\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137, 25184, 136820] the target: 14259\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137, 25184, 136820, 14259] the target: 47654\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137, 25184, 136820, 14259, 47654] the target: 189490\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137, 25184, 136820, 14259, 47654, 189490] the target: 89627\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137, 25184, 136820, 14259, 47654, 189490, 89627] the target: 20364\n",
      "when input is [35515, 201918, 103483, 20118, 189771, 154662, 20494, 154470, 157137, 25184, 136820, 14259, 47654, 189490, 89627, 20364] the target: 12014\n",
      "when input is [16747] the target: 32656\n",
      "when input is [16747, 32656] the target: 203052\n",
      "when input is [16747, 32656, 203052] the target: 158514\n",
      "when input is [16747, 32656, 203052, 158514] the target: 3196\n",
      "when input is [16747, 32656, 203052, 158514, 3196] the target: 181833\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833] the target: 62892\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892] the target: 159503\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503] the target: 151024\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024] the target: 155468\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024, 155468] the target: 164014\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024, 155468, 164014] the target: 21892\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024, 155468, 164014, 21892] the target: 44576\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024, 155468, 164014, 21892, 44576] the target: 98471\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024, 155468, 164014, 21892, 44576, 98471] the target: 77261\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024, 155468, 164014, 21892, 44576, 98471, 77261] the target: 111188\n",
      "when input is [16747, 32656, 203052, 158514, 3196, 181833, 62892, 159503, 151024, 155468, 164014, 21892, 44576, 98471, 77261, 111188] the target: 185358\n",
      "when input is [173717] the target: 60444\n",
      "when input is [173717, 60444] the target: 64945\n",
      "when input is [173717, 60444, 64945] the target: 92482\n",
      "when input is [173717, 60444, 64945, 92482] the target: 183435\n",
      "when input is [173717, 60444, 64945, 92482, 183435] the target: 27753\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753] the target: 50016\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016] the target: 29022\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022] the target: 40685\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685] the target: 161592\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685, 161592] the target: 190073\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685, 161592, 190073] the target: 25211\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685, 161592, 190073, 25211] the target: 175007\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685, 161592, 190073, 25211, 175007] the target: 170427\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685, 161592, 190073, 25211, 175007, 170427] the target: 171953\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685, 161592, 190073, 25211, 175007, 170427, 171953] the target: 186378\n",
      "when input is [173717, 60444, 64945, 92482, 183435, 27753, 50016, 29022, 40685, 161592, 190073, 25211, 175007, 170427, 171953, 186378] the target: 132523\n",
      "when input is [178682] the target: 87809\n",
      "when input is [178682, 87809] the target: 91316\n",
      "when input is [178682, 87809, 91316] the target: 16410\n",
      "when input is [178682, 87809, 91316, 16410] the target: 137440\n",
      "when input is [178682, 87809, 91316, 16410, 137440] the target: 59816\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816] the target: 69559\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559] the target: 71789\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789] the target: 48202\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202] the target: 98507\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202, 98507] the target: 19746\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202, 98507, 19746] the target: 72167\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202, 98507, 19746, 72167] the target: 107404\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202, 98507, 19746, 72167, 107404] the target: 143801\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202, 98507, 19746, 72167, 107404, 143801] the target: 94337\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202, 98507, 19746, 72167, 107404, 143801, 94337] the target: 44141\n",
      "when input is [178682, 87809, 91316, 16410, 137440, 59816, 69559, 71789, 48202, 98507, 19746, 72167, 107404, 143801, 94337, 44141] the target: 45515\n",
      "when input is [153346] the target: 34461\n",
      "when input is [153346, 34461] the target: 11816\n",
      "when input is [153346, 34461, 11816] the target: 46268\n",
      "when input is [153346, 34461, 11816, 46268] the target: 149008\n",
      "when input is [153346, 34461, 11816, 46268, 149008] the target: 105586\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586] the target: 88110\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110] the target: 152102\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102] the target: 129284\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284] the target: 45104\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284, 45104] the target: 6475\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284, 45104, 6475] the target: 104128\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284, 45104, 6475, 104128] the target: 33065\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284, 45104, 6475, 104128, 33065] the target: 134202\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284, 45104, 6475, 104128, 33065, 134202] the target: 36746\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284, 45104, 6475, 104128, 33065, 134202, 36746] the target: 202677\n",
      "when input is [153346, 34461, 11816, 46268, 149008, 105586, 88110, 152102, 129284, 45104, 6475, 104128, 33065, 134202, 36746, 202677] the target: 74803\n",
      "when input is [148764] the target: 151818\n",
      "when input is [148764, 151818] the target: 4889\n",
      "when input is [148764, 151818, 4889] the target: 39265\n",
      "when input is [148764, 151818, 4889, 39265] the target: 15323\n",
      "when input is [148764, 151818, 4889, 39265, 15323] the target: 139781\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781] the target: 7963\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963] the target: 22173\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173] the target: 100623\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623] the target: 87968\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623, 87968] the target: 178071\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623, 87968, 178071] the target: 154855\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623, 87968, 178071, 154855] the target: 106125\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623, 87968, 178071, 154855, 106125] the target: 62938\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623, 87968, 178071, 154855, 106125, 62938] the target: 170739\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623, 87968, 178071, 154855, 106125, 62938, 170739] the target: 59779\n",
      "when input is [148764, 151818, 4889, 39265, 15323, 139781, 7963, 22173, 100623, 87968, 178071, 154855, 106125, 62938, 170739, 59779] the target: 103522\n",
      "when input is [143871] the target: 89055\n",
      "when input is [143871, 89055] the target: 47063\n",
      "when input is [143871, 89055, 47063] the target: 112324\n",
      "when input is [143871, 89055, 47063, 112324] the target: 180639\n",
      "when input is [143871, 89055, 47063, 112324, 180639] the target: 72529\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529] the target: 76216\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216] the target: 180645\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645] the target: 35827\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827] the target: 162970\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827, 162970] the target: 57186\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827, 162970, 57186] the target: 149921\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827, 162970, 57186, 149921] the target: 187904\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827, 162970, 57186, 149921, 187904] the target: 66812\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827, 162970, 57186, 149921, 187904, 66812] the target: 33401\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827, 162970, 57186, 149921, 187904, 66812, 33401] the target: 92644\n",
      "when input is [143871, 89055, 47063, 112324, 180639, 72529, 76216, 180645, 35827, 162970, 57186, 149921, 187904, 66812, 33401, 92644] the target: 189436\n",
      "when input is [150910] the target: 129979\n",
      "when input is [150910, 129979] the target: 23654\n",
      "when input is [150910, 129979, 23654] the target: 21648\n",
      "when input is [150910, 129979, 23654, 21648] the target: 177742\n",
      "when input is [150910, 129979, 23654, 21648, 177742] the target: 192615\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615] the target: 98507\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507] the target: 198861\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861] the target: 77036\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036] the target: 178232\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036, 178232] the target: 16993\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036, 178232, 16993] the target: 150204\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036, 178232, 16993, 150204] the target: 80049\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036, 178232, 16993, 150204, 80049] the target: 99006\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036, 178232, 16993, 150204, 80049, 99006] the target: 98609\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036, 178232, 16993, 150204, 80049, 99006, 98609] the target: 27614\n",
      "when input is [150910, 129979, 23654, 21648, 177742, 192615, 98507, 198861, 77036, 178232, 16993, 150204, 80049, 99006, 98609, 27614] the target: 148434\n",
      "when input is [39820] the target: 48543\n",
      "when input is [39820, 48543] the target: 42660\n",
      "when input is [39820, 48543, 42660] the target: 201836\n",
      "when input is [39820, 48543, 42660, 201836] the target: 121170\n",
      "when input is [39820, 48543, 42660, 201836, 121170] the target: 87501\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501] the target: 96675\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675] the target: 129260\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260] the target: 148596\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596] the target: 154921\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596, 154921] the target: 195903\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596, 154921, 195903] the target: 160531\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596, 154921, 195903, 160531] the target: 26869\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596, 154921, 195903, 160531, 26869] the target: 94556\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596, 154921, 195903, 160531, 26869, 94556] the target: 96207\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596, 154921, 195903, 160531, 26869, 94556, 96207] the target: 54759\n",
      "when input is [39820, 48543, 42660, 201836, 121170, 87501, 96675, 129260, 148596, 154921, 195903, 160531, 26869, 94556, 96207, 54759] the target: 15598\n",
      "when input is [205630] the target: 112389\n",
      "when input is [205630, 112389] the target: 52252\n",
      "when input is [205630, 112389, 52252] the target: 29809\n",
      "when input is [205630, 112389, 52252, 29809] the target: 11645\n",
      "when input is [205630, 112389, 52252, 29809, 11645] the target: 28677\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677] the target: 97953\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953] the target: 129463\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463] the target: 185657\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657] the target: 84906\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657, 84906] the target: 162810\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657, 84906, 162810] the target: 106165\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657, 84906, 162810, 106165] the target: 142827\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657, 84906, 162810, 106165, 142827] the target: 164420\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657, 84906, 162810, 106165, 142827, 164420] the target: 68625\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657, 84906, 162810, 106165, 142827, 164420, 68625] the target: 27273\n",
      "when input is [205630, 112389, 52252, 29809, 11645, 28677, 97953, 129463, 185657, 84906, 162810, 106165, 142827, 164420, 68625, 27273] the target: 169003\n",
      "when input is [65315] the target: 42865\n",
      "when input is [65315, 42865] the target: 88347\n",
      "when input is [65315, 42865, 88347] the target: 85831\n",
      "when input is [65315, 42865, 88347, 85831] the target: 69411\n",
      "when input is [65315, 42865, 88347, 85831, 69411] the target: 122898\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898] the target: 112747\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747] the target: 169180\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180] the target: 119686\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686] the target: 201709\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686, 201709] the target: 81884\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686, 201709, 81884] the target: 34646\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686, 201709, 81884, 34646] the target: 132438\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686, 201709, 81884, 34646, 132438] the target: 194801\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686, 201709, 81884, 34646, 132438, 194801] the target: 16189\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686, 201709, 81884, 34646, 132438, 194801, 16189] the target: 23398\n",
      "when input is [65315, 42865, 88347, 85831, 69411, 122898, 112747, 169180, 119686, 201709, 81884, 34646, 132438, 194801, 16189, 23398] the target: 183747\n",
      "when input is [176572] the target: 129172\n",
      "when input is [176572, 129172] the target: 106154\n",
      "when input is [176572, 129172, 106154] the target: 42920\n",
      "when input is [176572, 129172, 106154, 42920] the target: 128661\n",
      "when input is [176572, 129172, 106154, 42920, 128661] the target: 37359\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359] the target: 82879\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879] the target: 162945\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945] the target: 12296\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296] the target: 92807\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296, 92807] the target: 182384\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296, 92807, 182384] the target: 47368\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296, 92807, 182384, 47368] the target: 12618\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296, 92807, 182384, 47368, 12618] the target: 47198\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296, 92807, 182384, 47368, 12618, 47198] the target: 162775\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296, 92807, 182384, 47368, 12618, 47198, 162775] the target: 102056\n",
      "when input is [176572, 129172, 106154, 42920, 128661, 37359, 82879, 162945, 12296, 92807, 182384, 47368, 12618, 47198, 162775, 102056] the target: 195155\n",
      "when input is [95616] the target: 103742\n",
      "when input is [95616, 103742] the target: 9922\n",
      "when input is [95616, 103742, 9922] the target: 86090\n",
      "when input is [95616, 103742, 9922, 86090] the target: 70113\n",
      "when input is [95616, 103742, 9922, 86090, 70113] the target: 108783\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783] the target: 18455\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455] the target: 110156\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156] the target: 100606\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606] the target: 142161\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606, 142161] the target: 49776\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606, 142161, 49776] the target: 135772\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606, 142161, 49776, 135772] the target: 117828\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606, 142161, 49776, 135772, 117828] the target: 165915\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606, 142161, 49776, 135772, 117828, 165915] the target: 61198\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606, 142161, 49776, 135772, 117828, 165915, 61198] the target: 77941\n",
      "when input is [95616, 103742, 9922, 86090, 70113, 108783, 18455, 110156, 100606, 142161, 49776, 135772, 117828, 165915, 61198, 77941] the target: 204809\n",
      "when input is [61370] the target: 34135\n",
      "when input is [61370, 34135] the target: 95263\n",
      "when input is [61370, 34135, 95263] the target: 188817\n",
      "when input is [61370, 34135, 95263, 188817] the target: 152738\n",
      "when input is [61370, 34135, 95263, 188817, 152738] the target: 144570\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570] the target: 106299\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299] the target: 31280\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280] the target: 768\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768] the target: 152102\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768, 152102] the target: 51966\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768, 152102, 51966] the target: 185175\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768, 152102, 51966, 185175] the target: 175103\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768, 152102, 51966, 185175, 175103] the target: 88965\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768, 152102, 51966, 185175, 175103, 88965] the target: 142206\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768, 152102, 51966, 185175, 175103, 88965, 142206] the target: 65398\n",
      "when input is [61370, 34135, 95263, 188817, 152738, 144570, 106299, 31280, 768, 152102, 51966, 185175, 175103, 88965, 142206, 65398] the target: 112013\n",
      "when input is [162870] the target: 50439\n",
      "when input is [162870, 50439] the target: 33755\n",
      "when input is [162870, 50439, 33755] the target: 60904\n",
      "when input is [162870, 50439, 33755, 60904] the target: 20392\n",
      "when input is [162870, 50439, 33755, 60904, 20392] the target: 75118\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118] the target: 126880\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880] the target: 9821\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821] the target: 60872\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872] the target: 179654\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872, 179654] the target: 138981\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872, 179654, 138981] the target: 117657\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872, 179654, 138981, 117657] the target: 54620\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872, 179654, 138981, 117657, 54620] the target: 28165\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872, 179654, 138981, 117657, 54620, 28165] the target: 151258\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872, 179654, 138981, 117657, 54620, 28165, 151258] the target: 195485\n",
      "when input is [162870, 50439, 33755, 60904, 20392, 75118, 126880, 9821, 60872, 179654, 138981, 117657, 54620, 28165, 151258, 195485] the target: 22482\n",
      "when input is [137026] the target: 121997\n",
      "when input is [137026, 121997] the target: 94097\n",
      "when input is [137026, 121997, 94097] the target: 100797\n",
      "when input is [137026, 121997, 94097, 100797] the target: 132323\n",
      "when input is [137026, 121997, 94097, 100797, 132323] the target: 97926\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926] the target: 193141\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141] the target: 86619\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619] the target: 97948\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948] the target: 91515\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948, 91515] the target: 23404\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948, 91515, 23404] the target: 69065\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948, 91515, 23404, 69065] the target: 98412\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948, 91515, 23404, 69065, 98412] the target: 7266\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948, 91515, 23404, 69065, 98412, 7266] the target: 152819\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948, 91515, 23404, 69065, 98412, 7266, 152819] the target: 166259\n",
      "when input is [137026, 121997, 94097, 100797, 132323, 97926, 193141, 86619, 97948, 91515, 23404, 69065, 98412, 7266, 152819, 166259] the target: 163313\n",
      "when input is [69597] the target: 153296\n",
      "when input is [69597, 153296] the target: 68280\n",
      "when input is [69597, 153296, 68280] the target: 64448\n",
      "when input is [69597, 153296, 68280, 64448] the target: 41379\n",
      "when input is [69597, 153296, 68280, 64448, 41379] the target: 73298\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298] the target: 56395\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395] the target: 50016\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016] the target: 205048\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048] the target: 46041\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048, 46041] the target: 188583\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048, 46041, 188583] the target: 143293\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048, 46041, 188583, 143293] the target: 192988\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048, 46041, 188583, 143293, 192988] the target: 160175\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048, 46041, 188583, 143293, 192988, 160175] the target: 66990\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048, 46041, 188583, 143293, 192988, 160175, 66990] the target: 3921\n",
      "when input is [69597, 153296, 68280, 64448, 41379, 73298, 56395, 50016, 205048, 46041, 188583, 143293, 192988, 160175, 66990, 3921] the target: 178682\n",
      "when input is [4358] the target: 870\n",
      "when input is [4358, 870] the target: 18848\n",
      "when input is [4358, 870, 18848] the target: 34375\n",
      "when input is [4358, 870, 18848, 34375] the target: 199819\n",
      "when input is [4358, 870, 18848, 34375, 199819] the target: 161744\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744] the target: 74019\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019] the target: 61882\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882] the target: 16078\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078] the target: 172847\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078, 172847] the target: 169003\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078, 172847, 169003] the target: 73109\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078, 172847, 169003, 73109] the target: 148014\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078, 172847, 169003, 73109, 148014] the target: 51369\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078, 172847, 169003, 73109, 148014, 51369] the target: 46881\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078, 172847, 169003, 73109, 148014, 51369, 46881] the target: 59152\n",
      "when input is [4358, 870, 18848, 34375, 199819, 161744, 74019, 61882, 16078, 172847, 169003, 73109, 148014, 51369, 46881, 59152] the target: 109654\n",
      "when input is [154039] the target: 125559\n",
      "when input is [154039, 125559] the target: 152651\n",
      "when input is [154039, 125559, 152651] the target: 26700\n",
      "when input is [154039, 125559, 152651, 26700] the target: 176449\n",
      "when input is [154039, 125559, 152651, 26700, 176449] the target: 170209\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209] the target: 193342\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342] the target: 73485\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485] the target: 95989\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989] the target: 186993\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989, 186993] the target: 79128\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989, 186993, 79128] the target: 47880\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989, 186993, 79128, 47880] the target: 82872\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989, 186993, 79128, 47880, 82872] the target: 171726\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989, 186993, 79128, 47880, 82872, 171726] the target: 195466\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989, 186993, 79128, 47880, 82872, 171726, 195466] the target: 84285\n",
      "when input is [154039, 125559, 152651, 26700, 176449, 170209, 193342, 73485, 95989, 186993, 79128, 47880, 82872, 171726, 195466, 84285] the target: 20709\n",
      "when input is [142406] the target: 13815\n",
      "when input is [142406, 13815] the target: 163699\n",
      "when input is [142406, 13815, 163699] the target: 202688\n",
      "when input is [142406, 13815, 163699, 202688] the target: 81043\n",
      "when input is [142406, 13815, 163699, 202688, 81043] the target: 10972\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972] the target: 142618\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618] the target: 165135\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135] the target: 125338\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338] the target: 39388\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338, 39388] the target: 52252\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338, 39388, 52252] the target: 53406\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338, 39388, 52252, 53406] the target: 176566\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338, 39388, 52252, 53406, 176566] the target: 130708\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338, 39388, 52252, 53406, 176566, 130708] the target: 157113\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338, 39388, 52252, 53406, 176566, 130708, 157113] the target: 46804\n",
      "when input is [142406, 13815, 163699, 202688, 81043, 10972, 142618, 165135, 125338, 39388, 52252, 53406, 176566, 130708, 157113, 46804] the target: 156731\n",
      "when input is [140425] the target: 200745\n",
      "when input is [140425, 200745] the target: 47022\n",
      "when input is [140425, 200745, 47022] the target: 121710\n",
      "when input is [140425, 200745, 47022, 121710] the target: 76036\n",
      "when input is [140425, 200745, 47022, 121710, 76036] the target: 126590\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590] the target: 171306\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306] the target: 167560\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560] the target: 83470\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470] the target: 165022\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470, 165022] the target: 192544\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470, 165022, 192544] the target: 84322\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470, 165022, 192544, 84322] the target: 144647\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470, 165022, 192544, 84322, 144647] the target: 83901\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470, 165022, 192544, 84322, 144647, 83901] the target: 146821\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470, 165022, 192544, 84322, 144647, 83901, 146821] the target: 195884\n",
      "when input is [140425, 200745, 47022, 121710, 76036, 126590, 171306, 167560, 83470, 165022, 192544, 84322, 144647, 83901, 146821, 195884] the target: 94350\n",
      "when input is [187824] the target: 99189\n",
      "when input is [187824, 99189] the target: 61035\n",
      "when input is [187824, 99189, 61035] the target: 480\n",
      "when input is [187824, 99189, 61035, 480] the target: 181226\n",
      "when input is [187824, 99189, 61035, 480, 181226] the target: 183712\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712] the target: 34778\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778] the target: 205196\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196] the target: 60735\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735] the target: 72476\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735, 72476] the target: 178951\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735, 72476, 178951] the target: 33145\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735, 72476, 178951, 33145] the target: 199058\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735, 72476, 178951, 33145, 199058] the target: 60272\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735, 72476, 178951, 33145, 199058, 60272] the target: 110422\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735, 72476, 178951, 33145, 199058, 60272, 110422] the target: 160097\n",
      "when input is [187824, 99189, 61035, 480, 181226, 183712, 34778, 205196, 60735, 72476, 178951, 33145, 199058, 60272, 110422, 160097] the target: 160097\n",
      "when input is [37142] the target: 101476\n",
      "when input is [37142, 101476] the target: 36508\n",
      "when input is [37142, 101476, 36508] the target: 139421\n",
      "when input is [37142, 101476, 36508, 139421] the target: 148971\n",
      "when input is [37142, 101476, 36508, 139421, 148971] the target: 62141\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141] the target: 72884\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884] the target: 173527\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527] the target: 171301\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301] the target: 106965\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301, 106965] the target: 116361\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301, 106965, 116361] the target: 170255\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301, 106965, 116361, 170255] the target: 177479\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301, 106965, 116361, 170255, 177479] the target: 95888\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301, 106965, 116361, 170255, 177479, 95888] the target: 199229\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301, 106965, 116361, 170255, 177479, 95888, 199229] the target: 107415\n",
      "when input is [37142, 101476, 36508, 139421, 148971, 62141, 72884, 173527, 171301, 106965, 116361, 170255, 177479, 95888, 199229, 107415] the target: 44297\n",
      "when input is [52575] the target: 139959\n",
      "when input is [52575, 139959] the target: 180534\n",
      "when input is [52575, 139959, 180534] the target: 99356\n",
      "when input is [52575, 139959, 180534, 99356] the target: 4969\n",
      "when input is [52575, 139959, 180534, 99356, 4969] the target: 176112\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112] the target: 159256\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256] the target: 106009\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009] the target: 139739\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739] the target: 66173\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739, 66173] the target: 69449\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739, 66173, 69449] the target: 153648\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739, 66173, 69449, 153648] the target: 96495\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739, 66173, 69449, 153648, 96495] the target: 46836\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739, 66173, 69449, 153648, 96495, 46836] the target: 166768\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739, 66173, 69449, 153648, 96495, 46836, 166768] the target: 102547\n",
      "when input is [52575, 139959, 180534, 99356, 4969, 176112, 159256, 106009, 139739, 66173, 69449, 153648, 96495, 46836, 166768, 102547] the target: 107153\n",
      "when input is [22515] the target: 80263\n",
      "when input is [22515, 80263] the target: 82669\n",
      "when input is [22515, 80263, 82669] the target: 188971\n",
      "when input is [22515, 80263, 82669, 188971] the target: 167316\n",
      "when input is [22515, 80263, 82669, 188971, 167316] the target: 186159\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159] the target: 46390\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390] the target: 19626\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626] the target: 52988\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988] the target: 67359\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988, 67359] the target: 184373\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988, 67359, 184373] the target: 167116\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988, 67359, 184373, 167116] the target: 148505\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988, 67359, 184373, 167116, 148505] the target: 56683\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988, 67359, 184373, 167116, 148505, 56683] the target: 106395\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988, 67359, 184373, 167116, 148505, 56683, 106395] the target: 9659\n",
      "when input is [22515, 80263, 82669, 188971, 167316, 186159, 46390, 19626, 52988, 67359, 184373, 167116, 148505, 56683, 106395, 9659] the target: 114214\n",
      "when input is [167433] the target: 111980\n",
      "when input is [167433, 111980] the target: 66634\n",
      "when input is [167433, 111980, 66634] the target: 102659\n",
      "when input is [167433, 111980, 66634, 102659] the target: 55551\n",
      "when input is [167433, 111980, 66634, 102659, 55551] the target: 199819\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819] the target: 108616\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616] the target: 100441\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441] the target: 72260\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260] the target: 150336\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260, 150336] the target: 184962\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260, 150336, 184962] the target: 129260\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260, 150336, 184962, 129260] the target: 12229\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260, 150336, 184962, 129260, 12229] the target: 29643\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260, 150336, 184962, 129260, 12229, 29643] the target: 138150\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260, 150336, 184962, 129260, 12229, 29643, 138150] the target: 117130\n",
      "when input is [167433, 111980, 66634, 102659, 55551, 199819, 108616, 100441, 72260, 150336, 184962, 129260, 12229, 29643, 138150, 117130] the target: 138145\n",
      "when input is [190170] the target: 97698\n",
      "when input is [190170, 97698] the target: 46761\n",
      "when input is [190170, 97698, 46761] the target: 69927\n",
      "when input is [190170, 97698, 46761, 69927] the target: 33969\n",
      "when input is [190170, 97698, 46761, 69927, 33969] the target: 134499\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499] the target: 116742\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742] the target: 40000\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000] the target: 110163\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163] the target: 15701\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163, 15701] the target: 172973\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163, 15701, 172973] the target: 201001\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163, 15701, 172973, 201001] the target: 55710\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163, 15701, 172973, 201001, 55710] the target: 88701\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163, 15701, 172973, 201001, 55710, 88701] the target: 104556\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163, 15701, 172973, 201001, 55710, 88701, 104556] the target: 188503\n",
      "when input is [190170, 97698, 46761, 69927, 33969, 134499, 116742, 40000, 110163, 15701, 172973, 201001, 55710, 88701, 104556, 188503] the target: 4814\n",
      "when input is [47339] the target: 151242\n",
      "when input is [47339, 151242] the target: 143119\n",
      "when input is [47339, 151242, 143119] the target: 94113\n",
      "when input is [47339, 151242, 143119, 94113] the target: 162839\n",
      "when input is [47339, 151242, 143119, 94113, 162839] the target: 89596\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596] the target: 98679\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679] the target: 53762\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762] the target: 8816\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816] the target: 139440\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816, 139440] the target: 195614\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816, 139440, 195614] the target: 17948\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816, 139440, 195614, 17948] the target: 39027\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816, 139440, 195614, 17948, 39027] the target: 132513\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816, 139440, 195614, 17948, 39027, 132513] the target: 33009\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816, 139440, 195614, 17948, 39027, 132513, 33009] the target: 121496\n",
      "when input is [47339, 151242, 143119, 94113, 162839, 89596, 98679, 53762, 8816, 139440, 195614, 17948, 39027, 132513, 33009, 121496] the target: 151033\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283.700491 M parameters\n"
     ]
    }
   ],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10002 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 12.4038, val loss 12.4044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10002 [06:50<9:01:46,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100: train loss 11.6052, val loss 11.6382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/10002 [14:37<8:50:41,  3.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200: train loss 10.9748, val loss 11.0017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/10002 [22:24<8:45:40,  3.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300: train loss 10.7388, val loss 10.7634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/10002 [30:11<8:40:09,  3.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400: train loss 10.6405, val loss 10.6572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 500/10002 [37:59<8:43:02,  3.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500: train loss 10.5756, val loss 10.5913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 600/10002 [45:59<8:35:00,  3.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600: train loss 10.5203, val loss 10.5539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 700/10002 [53:59<8:31:09,  3.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700: train loss 10.4527, val loss 10.5091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 800/10002 [1:02:00<8:27:06,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800: train loss 10.3732, val loss 10.4135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 900/10002 [1:10:00<8:22:44,  3.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900: train loss 10.2471, val loss 10.3297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1000/10002 [1:17:59<8:12:13,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000: train loss 10.1398, val loss 10.2081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1100/10002 [1:26:03<8:04:25,  3.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1100: train loss 10.0922, val loss 10.1346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1200/10002 [1:33:50<7:56:31,  3.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200: train loss 9.9997, val loss 10.1102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1300/10002 [1:41:37<7:51:37,  3.25s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1300: train loss 9.9402, val loss 10.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1400/10002 [1:49:26<7:49:45,  3.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1400: train loss 9.9091, val loss 10.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1500/10002 [1:57:13<7:41:22,  3.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500: train loss 9.8621, val loss 9.9556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1600/10002 [2:05:00<7:37:29,  3.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600: train loss 9.8076, val loss 9.8998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1700/10002 [2:12:46<7:27:52,  3.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1700: train loss 9.7422, val loss 9.8360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1800/10002 [2:20:32<7:25:51,  3.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1800: train loss 9.7269, val loss 9.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1900/10002 [2:28:19<7:17:07,  3.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1900: train loss 9.6903, val loss 9.7546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2000/10002 [2:36:07<7:18:04,  3.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000: train loss 9.6327, val loss 9.7111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2100/10002 [2:44:10<7:14:17,  3.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100: train loss 9.5666, val loss 9.6661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2200/10002 [2:52:12<7:07:55,  3.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2200: train loss 9.5390, val loss 9.6402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2300/10002 [3:00:12<7:02:52,  3.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2300: train loss 9.5016, val loss 9.6067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2400/10002 [3:08:13<6:58:57,  3.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2400: train loss 9.4590, val loss 9.5634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2500/10002 [3:16:14<6:50:36,  3.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500: train loss 9.3952, val loss 9.5383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2600/10002 [3:24:14<6:51:01,  3.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2600: train loss 9.3836, val loss 9.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2700/10002 [3:32:14<6:40:56,  3.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2700: train loss 9.3706, val loss 9.4594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2800/10002 [3:40:14<6:35:40,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2800: train loss 9.3263, val loss 9.4381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2900/10002 [3:48:25<6:30:07,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2900: train loss 9.2931, val loss 9.4085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3000/10002 [3:56:25<6:20:18,  3.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3000: train loss 9.2705, val loss 9.3785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3100/10002 [4:04:28<6:17:06,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3100: train loss 9.2200, val loss 9.3478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3200/10002 [4:12:28<6:11:05,  3.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3200: train loss 9.2273, val loss 9.3530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3300/10002 [4:20:28<6:07:18,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3300: train loss 9.1739, val loss 9.2881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3400/10002 [4:28:28<6:01:52,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3400: train loss 9.1725, val loss 9.2731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3500/10002 [4:36:29<5:57:40,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3500: train loss 9.1302, val loss 9.2625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3600/10002 [4:44:29<5:52:09,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3600: train loss 9.1241, val loss 9.2698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3700/10002 [4:52:31<5:46:08,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3700: train loss 9.1049, val loss 9.1797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3800/10002 [5:00:30<5:42:09,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3800: train loss 9.0759, val loss 9.2067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3900/10002 [5:08:30<5:34:04,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3900: train loss 9.0265, val loss 9.1780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 4000/10002 [5:16:30<5:29:54,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4000: train loss 9.0012, val loss 9.1210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4100/10002 [5:24:33<5:24:38,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4100: train loss 9.0341, val loss 9.1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4200/10002 [5:32:33<5:18:14,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4200: train loss 8.9892, val loss 9.1436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4300/10002 [5:40:34<5:11:55,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4300: train loss 8.9675, val loss 9.1033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4400/10002 [5:48:34<5:08:59,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4400: train loss 8.9553, val loss 9.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4500/10002 [5:56:35<5:01:17,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4500: train loss 8.9232, val loss 9.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4600/10002 [6:04:36<4:55:00,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4600: train loss 8.9291, val loss 9.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4700/10002 [6:12:36<4:51:59,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4700: train loss 8.8952, val loss 9.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4800/10002 [6:20:37<4:46:58,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4800: train loss 8.8546, val loss 9.0344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4900/10002 [6:28:38<4:43:05,  3.33s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4900: train loss 8.8646, val loss 8.9839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5000/10002 [6:36:38<4:34:45,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5000: train loss 8.8237, val loss 9.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5100/10002 [6:44:41<4:30:04,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5100: train loss 8.8137, val loss 8.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5200/10002 [6:52:42<4:25:02,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5200: train loss 8.8275, val loss 8.9687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5300/10002 [7:00:42<4:17:29,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5300: train loss 8.8118, val loss 8.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5400/10002 [7:08:43<4:13:04,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5400: train loss 8.7688, val loss 8.9499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 5500/10002 [7:16:43<4:07:13,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5500: train loss 8.7640, val loss 8.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5600/10002 [7:24:44<4:00:05,  3.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5600: train loss 8.7204, val loss 8.9017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 5700/10002 [7:32:45<3:56:37,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5700: train loss 8.7169, val loss 8.8952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 5800/10002 [7:40:45<3:50:30,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5800: train loss 8.7133, val loss 8.8707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5900/10002 [7:48:46<3:45:31,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5900: train loss 8.7047, val loss 8.8488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 6000/10002 [7:56:45<3:40:42,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6000: train loss 8.6807, val loss 8.8315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6100/10002 [8:04:48<3:33:31,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6100: train loss 8.6701, val loss 8.8227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 6200/10002 [8:12:48<3:27:30,  3.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6200: train loss 8.6508, val loss 8.8069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 6300/10002 [8:20:49<3:23:40,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6300: train loss 8.6181, val loss 8.7954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 6400/10002 [8:28:49<3:18:14,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6400: train loss 8.6148, val loss 8.7719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 6500/10002 [8:36:50<3:13:28,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6500: train loss 8.5883, val loss 8.7709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 6600/10002 [8:44:51<3:06:42,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6600: train loss 8.5786, val loss 8.7409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6700/10002 [8:52:51<3:01:00,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6700: train loss 8.5611, val loss 8.7205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 6800/10002 [9:00:52<2:56:02,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6800: train loss 8.5615, val loss 8.7130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 6900/10002 [9:08:53<2:51:05,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6900: train loss 8.5318, val loss 8.6980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 7000/10002 [9:16:53<2:46:14,  3.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7000: train loss 8.5172, val loss 8.6947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 7100/10002 [9:24:56<2:40:03,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7100: train loss 8.5096, val loss 8.6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 7200/10002 [9:32:56<2:34:36,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7200: train loss 8.5000, val loss 8.6758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 7300/10002 [9:40:57<2:27:44,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7300: train loss 8.5243, val loss 8.6766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 7400/10002 [9:49:00<2:23:06,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7400: train loss 8.4945, val loss 8.6589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 7500/10002 [9:57:00<2:16:41,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7500: train loss 8.4650, val loss 8.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 7600/10002 [10:05:00<2:10:43,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7600: train loss 8.4563, val loss 8.6571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 7700/10002 [10:13:01<2:06:37,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7700: train loss 8.4847, val loss 8.6545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7800/10002 [10:21:01<2:01:04,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7800: train loss 8.4264, val loss 8.6442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 7900/10002 [10:29:04<1:55:57,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7900: train loss 8.4539, val loss 8.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 8000/10002 [10:37:04<1:49:41,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8000: train loss 8.4264, val loss 8.5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 8100/10002 [10:45:06<1:43:42,  3.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8100: train loss 8.4235, val loss 8.5834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 8200/10002 [10:53:07<1:39:22,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8200: train loss 8.4163, val loss 8.5839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 8300/10002 [11:01:07<1:33:04,  3.28s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8300: train loss 8.4076, val loss 8.5767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 8400/10002 [11:09:08<1:28:19,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8400: train loss 8.3626, val loss 8.5611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 8500/10002 [11:17:09<1:22:16,  3.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8500: train loss 8.3840, val loss 8.6015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 8600/10002 [11:25:10<1:17:26,  3.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8600: train loss 8.3916, val loss 8.5856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 8700/10002 [11:33:08<1:10:31,  3.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8700: train loss 8.3661, val loss 8.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 8800/10002 [11:40:55<1:05:23,  3.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8800: train loss 8.3785, val loss 8.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8900/10002 [11:48:41<1:00:01,  3.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8900: train loss 8.3706, val loss 8.5773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 9000/10002 [11:56:28<54:37,  3.27s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9000: train loss 8.3908, val loss 8.5529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 9100/10002 [12:04:17<49:20,  3.28s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9100: train loss 8.3626, val loss 8.5171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 9200/10002 [12:12:03<43:36,  3.26s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9200: train loss 8.3661, val loss 8.5493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 9300/10002 [12:19:50<38:19,  3.28s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9300: train loss 8.3675, val loss 8.5602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 9400/10002 [12:27:36<32:49,  3.27s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9400: train loss 8.3879, val loss 8.5428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 9500/10002 [12:35:23<27:27,  3.28s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9500: train loss 8.3821, val loss 8.5504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 9600/10002 [12:43:12<22:08,  3.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9600: train loss 8.3517, val loss 8.5289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 9700/10002 [12:51:12<16:33,  3.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9700: train loss 8.3636, val loss 8.5693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9800/10002 [12:59:13<11:04,  3.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9800: train loss 8.3643, val loss 8.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 9900/10002 [13:07:13<05:37,  3.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9900: train loss 8.3665, val loss 8.5484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9999/10002 [13:15:10<00:09,  3.31s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9999: train loss 8.3756, val loss 8.5612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 10000/10002 [13:17:44<01:36, 48.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000: train loss 8.3954, val loss 8.5737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10002/10002 [13:20:24<00:00,  4.80s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, max_iters)\n",
    "\n",
    "for iter in tqdm(range(max_iters + 2)):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    if (iter % 1000 == 0) and (iter > 0):\n",
    "        torch.save(model.state_dict(), f\"model_{iter}.pth\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_txt = '''0%|          | 0/10002 [00:00<?, ?it/s]\n",
    "step 0: train loss 12.4038, val loss 12.4044\n",
    "  1%|          | 100/10002 [06:50<9:01:46,  3.28s/it]\n",
    "step 100: train loss 11.6052, val loss 11.6382\n",
    "  2%|▏         | 200/10002 [14:37<8:50:41,  3.25s/it]  \n",
    "step 200: train loss 10.9748, val loss 11.0017\n",
    "  3%|▎         | 300/10002 [22:24<8:45:40,  3.25s/it]  \n",
    "step 300: train loss 10.7388, val loss 10.7634\n",
    "  4%|▍         | 400/10002 [30:11<8:40:09,  3.25s/it]  \n",
    "step 400: train loss 10.6405, val loss 10.6572\n",
    "  5%|▍         | 500/10002 [37:59<8:43:02,  3.30s/it]  \n",
    "step 500: train loss 10.5756, val loss 10.5913\n",
    "  6%|▌         | 600/10002 [45:59<8:35:00,  3.29s/it]  \n",
    "step 600: train loss 10.5203, val loss 10.5539\n",
    "  7%|▋         | 700/10002 [53:59<8:31:09,  3.30s/it]  \n",
    "step 700: train loss 10.4527, val loss 10.5091\n",
    "  8%|▊         | 800/10002 [1:02:00<8:27:06,  3.31s/it]\n",
    "step 800: train loss 10.3732, val loss 10.4135\n",
    "  9%|▉         | 900/10002 [1:10:00<8:22:44,  3.31s/it]  \n",
    "step 900: train loss 10.2471, val loss 10.3297\n",
    " 10%|▉         | 1000/10002 [1:17:59<8:12:13,  3.28s/it] \n",
    "step 1000: train loss 10.1398, val loss 10.2081\n",
    " 11%|█         | 1100/10002 [1:26:03<8:04:25,  3.27s/it]  \n",
    "step 1100: train loss 10.0922, val loss 10.1346\n",
    " 12%|█▏        | 1200/10002 [1:33:50<7:56:31,  3.25s/it]  \n",
    "step 1200: train loss 9.9997, val loss 10.1102\n",
    " 13%|█▎        | 1300/10002 [1:41:37<7:51:37,  3.25s/it]  \n",
    "step 1300: train loss 9.9402, val loss 10.0421\n",
    " 14%|█▍        | 1400/10002 [1:49:26<7:49:45,  3.28s/it]  \n",
    "step 1400: train loss 9.9091, val loss 10.0081\n",
    " 15%|█▍        | 1500/10002 [1:57:13<7:41:22,  3.26s/it]  \n",
    "step 1500: train loss 9.8621, val loss 9.9556\n",
    " 16%|█▌        | 1600/10002 [2:05:00<7:37:29,  3.27s/it]  \n",
    "step 1600: train loss 9.8076, val loss 9.8998\n",
    " 17%|█▋        | 1700/10002 [2:12:46<7:27:52,  3.24s/it]  \n",
    "step 1700: train loss 9.7422, val loss 9.8360\n",
    " 18%|█▊        | 1800/10002 [2:20:32<7:25:51,  3.26s/it]  \n",
    "step 1800: train loss 9.7269, val loss 9.8189\n",
    " 19%|█▉        | 1900/10002 [2:28:19<7:17:07,  3.24s/it]  \n",
    "step 1900: train loss 9.6903, val loss 9.7546\n",
    " 20%|█▉        | 2000/10002 [2:36:07<7:18:04,  3.28s/it]  \n",
    "step 2000: train loss 9.6327, val loss 9.7111\n",
    " 21%|██        | 2100/10002 [2:44:10<7:14:17,  3.30s/it]  \n",
    "step 2100: train loss 9.5666, val loss 9.6661\n",
    " 22%|██▏       | 2200/10002 [2:52:12<7:07:55,  3.29s/it]  \n",
    "step 2200: train loss 9.5390, val loss 9.6402\n",
    " 23%|██▎       | 2300/10002 [3:00:12<7:02:52,  3.29s/it]  \n",
    "step 2300: train loss 9.5016, val loss 9.6067\n",
    " 24%|██▍       | 2400/10002 [3:08:13<6:58:57,  3.31s/it]  \n",
    "step 2400: train loss 9.4590, val loss 9.5634\n",
    " 25%|██▍       | 2500/10002 [3:16:14<6:50:36,  3.28s/it]  \n",
    "step 2500: train loss 9.3952, val loss 9.5383\n",
    " 26%|██▌       | 2600/10002 [3:24:14<6:51:01,  3.33s/it]  \n",
    "step 2600: train loss 9.3836, val loss 9.5173\n",
    " 27%|██▋       | 2700/10002 [3:32:14<6:40:56,  3.29s/it]  \n",
    "step 2700: train loss 9.3706, val loss 9.4594\n",
    " 28%|██▊       | 2800/10002 [3:40:14<6:35:40,  3.30s/it] \n",
    "step 2800: train loss 9.3263, val loss 9.4381\n",
    " 29%|██▉       | 2900/10002 [3:48:25<6:30:07,  3.30s/it] \n",
    "step 2900: train loss 9.2931, val loss 9.4085\n",
    " 30%|██▉       | 3000/10002 [3:56:25<6:20:18,  3.26s/it] \n",
    "step 3000: train loss 9.2705, val loss 9.3785\n",
    " 31%|███       | 3100/10002 [4:04:28<6:17:06,  3.28s/it] \n",
    "step 3100: train loss 9.2200, val loss 9.3478\n",
    " 32%|███▏      | 3200/10002 [4:12:28<6:11:05,  3.27s/it] \n",
    "step 3200: train loss 9.2273, val loss 9.3530\n",
    " 33%|███▎      | 3300/10002 [4:20:28<6:07:18,  3.29s/it] \n",
    "step 3300: train loss 9.1739, val loss 9.2881\n",
    " 34%|███▍      | 3400/10002 [4:28:28<6:01:52,  3.29s/it] \n",
    "step 3400: train loss 9.1725, val loss 9.2731\n",
    " 35%|███▍      | 3500/10002 [4:36:29<5:57:40,  3.30s/it] \n",
    "step 3500: train loss 9.1302, val loss 9.2625\n",
    " 36%|███▌      | 3600/10002 [4:44:29<5:52:09,  3.30s/it] \n",
    "step 3600: train loss 9.1241, val loss 9.2698\n",
    " 37%|███▋      | 3700/10002 [4:52:31<5:46:08,  3.30s/it] \n",
    "step 3700: train loss 9.1049, val loss 9.1797\n",
    " 38%|███▊      | 3800/10002 [5:00:30<5:42:09,  3.31s/it] \n",
    "step 3800: train loss 9.0759, val loss 9.2067\n",
    " 39%|███▉      | 3900/10002 [5:08:30<5:34:04,  3.28s/it] \n",
    "step 3900: train loss 9.0265, val loss 9.1780\n",
    " 40%|███▉      | 4000/10002 [5:16:30<5:29:54,  3.30s/it] \n",
    "step 4000: train loss 9.0012, val loss 9.1210\n",
    " 41%|████      | 4100/10002 [5:24:33<5:24:38,  3.30s/it] \n",
    "step 4100: train loss 9.0341, val loss 9.1587\n",
    " 42%|████▏     | 4200/10002 [5:32:33<5:18:14,  3.29s/it] \n",
    "step 4200: train loss 8.9892, val loss 9.1436\n",
    " 43%|████▎     | 4300/10002 [5:40:34<5:11:55,  3.28s/it] \n",
    "step 4300: train loss 8.9675, val loss 9.1033\n",
    " 44%|████▍     | 4400/10002 [5:48:34<5:08:59,  3.31s/it] \n",
    "step 4400: train loss 8.9553, val loss 9.0786\n",
    " 45%|████▍     | 4500/10002 [5:56:35<5:01:17,  3.29s/it] \n",
    "step 4500: train loss 8.9232, val loss 9.0819\n",
    " 46%|████▌     | 4600/10002 [6:04:36<4:55:00,  3.28s/it] \n",
    "step 4600: train loss 8.9291, val loss 9.0713\n",
    " 47%|████▋     | 4700/10002 [6:12:36<4:51:59,  3.30s/it] \n",
    "step 4700: train loss 8.8952, val loss 9.0229\n",
    " 48%|████▊     | 4800/10002 [6:20:37<4:46:58,  3.31s/it] \n",
    "step 4800: train loss 8.8546, val loss 9.0344\n",
    " 49%|████▉     | 4900/10002 [6:28:38<4:43:05,  3.33s/it] \n",
    "step 4900: train loss 8.8646, val loss 8.9839\n",
    " 50%|████▉     | 5000/10002 [6:36:38<4:34:45,  3.30s/it] \n",
    "step 5000: train loss 8.8237, val loss 9.0121\n",
    " 51%|█████     | 5100/10002 [6:44:41<4:30:04,  3.31s/it] \n",
    "step 5100: train loss 8.8137, val loss 8.9554\n",
    " 52%|█████▏    | 5200/10002 [6:52:42<4:25:02,  3.31s/it] \n",
    "step 5200: train loss 8.8275, val loss 8.9687\n",
    " 53%|█████▎    | 5300/10002 [7:00:42<4:17:29,  3.29s/it] \n",
    "step 5300: train loss 8.8118, val loss 8.9451\n",
    " 54%|█████▍    | 5400/10002 [7:08:43<4:13:04,  3.30s/it] \n",
    "step 5400: train loss 8.7688, val loss 8.9499\n",
    " 55%|█████▍    | 5500/10002 [7:16:43<4:07:13,  3.29s/it] \n",
    "step 5500: train loss 8.7640, val loss 8.8946\n",
    " 56%|█████▌    | 5600/10002 [7:24:44<4:00:05,  3.27s/it] \n",
    "step 5600: train loss 8.7204, val loss 8.9017\n",
    " 57%|█████▋    | 5700/10002 [7:32:45<3:56:37,  3.30s/it] \n",
    "step 5700: train loss 8.7169, val loss 8.8952\n",
    " 58%|█████▊    | 5800/10002 [7:40:45<3:50:30,  3.29s/it] \n",
    "step 5800: train loss 8.7133, val loss 8.8707\n",
    " 59%|█████▉    | 5900/10002 [7:48:46<3:45:31,  3.30s/it] \n",
    "step 5900: train loss 8.7047, val loss 8.8488\n",
    " 60%|█████▉    | 6000/10002 [7:56:45<3:40:42,  3.31s/it] \n",
    "step 6000: train loss 8.6807, val loss 8.8315\n",
    " 61%|██████    | 6100/10002 [8:04:48<3:33:31,  3.28s/it] \n",
    "step 6100: train loss 8.6701, val loss 8.8227\n",
    " 62%|██████▏   | 6200/10002 [8:12:48<3:27:30,  3.27s/it] \n",
    "step 6200: train loss 8.6508, val loss 8.8069\n",
    " 63%|██████▎   | 6300/10002 [8:20:49<3:23:40,  3.30s/it] \n",
    "step 6300: train loss 8.6181, val loss 8.7954\n",
    " 64%|██████▍   | 6400/10002 [8:28:49<3:18:14,  3.30s/it] \n",
    "step 6400: train loss 8.6148, val loss 8.7719\n",
    " 65%|██████▍   | 6500/10002 [8:36:50<3:13:28,  3.31s/it] \n",
    "step 6500: train loss 8.5883, val loss 8.7709\n",
    " 66%|██████▌   | 6600/10002 [8:44:51<3:06:42,  3.29s/it] \n",
    "step 6600: train loss 8.5786, val loss 8.7409\n",
    " 67%|██████▋   | 6700/10002 [8:52:51<3:01:00,  3.29s/it] \n",
    "step 6700: train loss 8.5611, val loss 8.7205\n",
    " 68%|██████▊   | 6800/10002 [9:00:52<2:56:02,  3.30s/it] \n",
    "step 6800: train loss 8.5615, val loss 8.7130\n",
    " 69%|██████▉   | 6900/10002 [9:08:53<2:51:05,  3.31s/it] \n",
    "step 6900: train loss 8.5318, val loss 8.6980\n",
    " 70%|██████▉   | 7000/10002 [9:16:53<2:46:14,  3.32s/it] \n",
    "step 7000: train loss 8.5172, val loss 8.6947\n",
    " 71%|███████   | 7100/10002 [9:24:56<2:40:03,  3.31s/it] \n",
    "step 7100: train loss 8.5096, val loss 8.6732\n",
    " 72%|███████▏  | 7200/10002 [9:32:56<2:34:36,  3.31s/it] \n",
    "step 7200: train loss 8.5000, val loss 8.6758\n",
    " 73%|███████▎  | 7300/10002 [9:40:57<2:27:44,  3.28s/it] \n",
    "step 7300: train loss 8.5243, val loss 8.6766\n",
    " 74%|███████▍  | 7400/10002 [9:49:00<2:23:06,  3.30s/it] \n",
    "step 7400: train loss 8.4945, val loss 8.6589\n",
    " 75%|███████▍  | 7500/10002 [9:57:00<2:16:41,  3.28s/it] \n",
    "step 7500: train loss 8.4650, val loss 8.6569\n",
    " 76%|███████▌  | 7600/10002 [10:05:00<2:10:43,  3.27s/it]\n",
    "step 7600: train loss 8.4563, val loss 8.6571\n",
    " 77%|███████▋  | 7700/10002 [10:13:01<2:06:37,  3.30s/it] \n",
    "step 7700: train loss 8.4847, val loss 8.6545\n",
    " 78%|███████▊  | 7800/10002 [10:21:01<2:01:04,  3.30s/it] \n",
    "step 7800: train loss 8.4264, val loss 8.6442\n",
    " 79%|███████▉  | 7900/10002 [10:29:04<1:55:57,  3.31s/it] \n",
    "step 7900: train loss 8.4539, val loss 8.6261\n",
    " 80%|███████▉  | 8000/10002 [10:37:04<1:49:41,  3.29s/it] \n",
    "step 8000: train loss 8.4264, val loss 8.5978\n",
    " 81%|████████  | 8100/10002 [10:45:06<1:43:42,  3.27s/it] \n",
    "step 8100: train loss 8.4235, val loss 8.5834\n",
    " 82%|████████▏ | 8200/10002 [10:53:07<1:39:22,  3.31s/it] \n",
    "step 8200: train loss 8.4163, val loss 8.5839\n",
    " 83%|████████▎ | 8300/10002 [11:01:07<1:33:04,  3.28s/it] \n",
    "step 8300: train loss 8.4076, val loss 8.5767\n",
    " 84%|████████▍ | 8400/10002 [11:09:08<1:28:19,  3.31s/it] \n",
    "step 8400: train loss 8.3626, val loss 8.5611\n",
    " 85%|████████▍ | 8500/10002 [11:17:09<1:22:16,  3.29s/it] \n",
    "step 8500: train loss 8.3840, val loss 8.6015\n",
    " 86%|████████▌ | 8600/10002 [11:25:10<1:17:26,  3.31s/it] \n",
    "step 8600: train loss 8.3916, val loss 8.5856\n",
    " 87%|████████▋ | 8700/10002 [11:33:08<1:10:31,  3.25s/it] \n",
    "step 8700: train loss 8.3661, val loss 8.5698\n",
    " 88%|████████▊ | 8800/10002 [11:40:55<1:05:23,  3.26s/it] \n",
    "step 8800: train loss 8.3785, val loss 8.5679\n",
    " 89%|████████▉ | 8900/10002 [11:48:41<1:00:01,  3.27s/it] \n",
    "step 8900: train loss 8.3706, val loss 8.5773\n",
    " 90%|████████▉ | 9000/10002 [11:56:28<54:37,  3.27s/it]   \n",
    "step 9000: train loss 8.3908, val loss 8.5529\n",
    " 91%|█████████ | 9100/10002 [12:04:17<49:20,  3.28s/it]   \n",
    "step 9100: train loss 8.3626, val loss 8.5171\n",
    " 92%|█████████▏| 9200/10002 [12:12:03<43:36,  3.26s/it]   \n",
    "step 9200: train loss 8.3661, val loss 8.5493\n",
    " 93%|█████████▎| 9300/10002 [12:19:50<38:19,  3.28s/it]   \n",
    "step 9300: train loss 8.3675, val loss 8.5602\n",
    " 94%|█████████▍| 9400/10002 [12:27:36<32:49,  3.27s/it]  \n",
    "step 9400: train loss 8.3879, val loss 8.5428\n",
    " 95%|█████████▍| 9500/10002 [12:35:23<27:27,  3.28s/it]  \n",
    "step 9500: train loss 8.3821, val loss 8.5504\n",
    " 96%|█████████▌| 9600/10002 [12:43:12<22:08,  3.31s/it]  \n",
    "step 9600: train loss 8.3517, val loss 8.5289\n",
    " 97%|█████████▋| 9700/10002 [12:51:12<16:33,  3.29s/it]  \n",
    "step 9700: train loss 8.3636, val loss 8.5693\n",
    " 98%|█████████▊| 9800/10002 [12:59:13<11:04,  3.29s/it]  \n",
    "step 9800: train loss 8.3643, val loss 8.5738\n",
    " 99%|█████████▉| 9900/10002 [13:07:13<05:37,  3.30s/it]  \n",
    "step 9900: train loss 8.3665, val loss 8.5484\n",
    "100%|█████████▉| 9999/10002 [13:15:10<00:09,  3.31s/it]  \n",
    "step 9999: train loss 8.3756, val loss 8.5612\n",
    "100%|█████████▉| 10000/10002 [13:17:44<01:36, 48.47s/it]\n",
    "step 10000: train loss 8.3954, val loss 8.5737\n",
    "100%|██████████| 10002/10002 [13:20:24<00:00,  4.80s/it]'''\n",
    "# skip every other line\n",
    "training_txt = training_txt.split('\\n')[1::2]\n",
    "training_train_loss = [float(x.split(' ')[4][:-1]) for x in training_txt]\n",
    "training_val_loss = [float(x.split(' ')[7]) for x in training_txt]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training_train_loss, label='train')\n",
    "plt.plot(training_val_loss, label='val')\n",
    "plt.legend()\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Training curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spotify:track:000VZqvXwT0YNqKk7iG2GS', 'spotify:track:1lT2e20QWYB10Upc5LMfZL', 'spotify:track:1KL5oNbR2LJC5Rujsp4LCu', 'spotify:track:6NIQ1gs2VTJ41oTuUPOXPp', 'spotify:track:1iv51ZWIQec2BV7LCR5ynP', 'spotify:track:6EX2Dw0TanGWnJGBmDIFZe', 'spotify:track:4mGpycaal3DkIY04utgNVz', 'spotify:track:51ywMlgjW0YZvAkhaH82zL', 'spotify:track:5QloliQBgc3oQoe7cuXegW', 'spotify:track:5ps5dkx2ifvBSnA8RP5qTS', 'spotify:track:6sH2pWmmPgcFZVcEX48FUw', 'spotify:track:2OG73Fda8Qc0zLbq5CuUuq', 'spotify:track:0FqK5zRZm46125vbLR7K6v', 'spotify:track:5MlNpUqxIDwgsCjugiGxuW', 'spotify:track:2sCiFThZI8pAs39foSJ3ns', 'spotify:track:5wQ61GS6nT8DZaxtqKOesw', 'spotify:track:5HsAR5wajzn4JideTD4SXs', 'spotify:track:49C3rdFWwDfKshJfnumXOR', 'spotify:track:34DE20PRYUJ1QlT7h2Mq3W', 'spotify:track:5Ycx0AGDOp2el1rVuW3KqV', 'spotify:track:2Gsuc5EdHYPGK6h0xu11tX', 'spotify:track:1nMYtxDrONcoGnKRvxTwPv', 'spotify:track:4wAjmojVxc6Wbeca9XvnDo', 'spotify:track:6PaphYrUMHd8cgnw4TqqrC', 'spotify:track:6Ihd80oDuSg0w81nq8reOT', 'spotify:track:5gLtErihORx8JzySyMWOSC', 'spotify:track:6Wi7A64puKsOA8lOYZJ1rx', 'spotify:track:1jcTfIiBUz35bBqgHYItP6', 'spotify:track:78n4VGVg8ACbCznXtF7wqD', 'spotify:track:1mqyxQ28ohNqEM8bXEeSwu', 'spotify:track:3JVI710efRiB7comD33vD1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=30)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(206091, 640)\n",
       "  (position_embedding_table): Embedding(16, 640)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-19): 20 x Head(\n",
       "            (key): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=640, out_features=2560, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (3): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-19): 20 x Head(\n",
       "            (key): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=640, out_features=2560, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (3): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-19): 20 x Head(\n",
       "            (key): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=640, out_features=2560, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (3): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-19): 20 x Head(\n",
       "            (key): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (query): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (value): Linear(in_features=640, out_features=32, bias=False)\n",
       "            (dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedFoward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=640, out_features=2560, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "          (3): Dropout(p=0.05, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=640, out_features=206091, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../embeddings/weights/'\n",
    "track_uri_to_track_name = json.load(open(path + 'track_uri_to_track_name.json'))\n",
    "track_uri_to_artist_name = json.load(open(path + 'track_uri_to_artist_name.json'))\n",
    "track_uri_to_album_name = json.load(open(path + 'track_uri_to_album_name.json'))\n",
    "\n",
    "m = BigramLanguageModel()\n",
    "m.load_state_dict(torch.load('model_10000.pth'))\n",
    "m.to(device)\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diet Mountain Dew by Lana Del Rey from the album Born To Die - The Paradise Edition\n",
      "Francis Forever by Mitski from the album Bury Me At Makeout Creek\n",
      "Lovers Rock by TV Girl from the album French Exit\n",
      "===========\n",
      "Generated playlist:\n",
      "Diet Mountain Dew by Lana Del Rey from the album Born To Die - The Paradise Edition\n",
      "Francis Forever by Mitski from the album Bury Me At Makeout Creek\n",
      "Lovers Rock by TV Girl from the album French Exit\n",
      "K-LOVE Fan Awards: Songs of the Year (2015 Mash-Up) by Anthem Lights from the album K-LOVE Fan Awards: Songs of the Year (2015 Mash-Up)\n",
      "Million Dollar Bills by Lorde from the album Pure Heroine\n",
      "The Real World by Drugdealer from the album The End Of Comedy\n",
      "All Creatures (Live) by Kings Kaleidoscope from the album Live in Focus - EP\n",
      "Reflection - From \"Mulan\"/Soundtrack Version by Lea Salonga from the album Mulan\n",
      "When Will My Life Begin - From \"Tangled\"/Soundtrack Version by Mandy Moore from the album Tangled\n",
      "A Whole New World by Lea Salonga from the album Aladdin\n",
      "I Just Can't Wait to Be King - From \"The Lion King\"/Soundtrack Version by Jason Weaver from the album The Lion King\n",
      "Poor Unfortunate Souls - From \"The Little Mermaid\" / Soundtrack Version by Pat Carroll from the album Little Mermaid\n",
      "I Just Can't Wait to Be King - From \"The Lion King\"/Soundtrack Version by Jason Weaver from the album The Lion King\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_playlist = [\n",
    "    'spotify:track:2vtmY2mSccRzKGjtcHSzI3',\n",
    "    'spotify:track:6gpSU2C1VJq0xaVIGtQPVi',\n",
    "    'spotify:track:1H7zdcRD0gLGQY0w5ejGgX',\n",
    "]\n",
    "\n",
    "for track_uri in first_playlist:\n",
    "    track_name = track_uri_to_track_name[track_uri]\n",
    "    artist_name = track_uri_to_artist_name[track_uri]\n",
    "    album_name = track_uri_to_album_name[track_uri]\n",
    "    print(f\"{track_name} by {artist_name} from the album {album_name}\")\n",
    "\n",
    "encoded_playlist = encode(first_playlist)\n",
    "context = torch.tensor(encoded_playlist, dtype=torch.long, device=device).unsqueeze(0).to(device)\n",
    "generated_playlist = decode(m.generate(context, max_new_tokens=10)[0].tolist())\n",
    "print('===========')\n",
    "print('Generated playlist:')\n",
    "for track_uri in generated_playlist:\n",
    "    track_name = track_uri_to_track_name[track_uri]\n",
    "    artist_name = track_uri_to_artist_name[track_uri]\n",
    "    album_name = track_uri_to_album_name[track_uri]\n",
    "    print(f\"{track_name} by {artist_name} from the album {album_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sympathy For The Devil by The Rolling Stones from the album Beggars Banquet\n",
      "Stairway To Heaven by Led Zeppelin from the album Led Zeppelin IV\n",
      "Sultans Of Swing by Dire Straits from the album Dire Straits\n",
      "===========\n",
      "Generated playlist:\n",
      "Sympathy For The Devil by The Rolling Stones from the album Beggars Banquet\n",
      "Stairway To Heaven by Led Zeppelin from the album Led Zeppelin IV\n",
      "Sultans Of Swing by Dire Straits from the album Dire Straits\n",
      "Only the Good Die Young by Billy Joel from the album The Stranger (30th Anniversary Legacy Edition)\n",
      "Aerials by System Of A Down from the album Toxicity\n",
      "Eye Of The Beholder by Metallica from the album ...And Justice For All\n",
      "Get Closer by Seals and Crofts from the album Get Closer\n",
      "Listen To The Music by The Doobie Brothers from the album Toulouse Street\n",
      "I'm Free by The Who from the album Tommy\n",
      "Heat Of The Moment by Asia from the album Asia\n",
      "Right Here Waiting by Richard Marx from the album Repeat Offender\n",
      "Paradise City by Guns N' Roses from the album Appetite For Destruction\n",
      "Hot For Teacher - 2015 Remastered Version by Van Halen from the album 1984\n"
     ]
    }
   ],
   "source": [
    "second_playlist = [\n",
    "    'spotify:track:4sFbojhVXQv7dBC9PVCcRn',\n",
    "    'spotify:track:5CQ30WqJwcep0pYcV4AMNc',\n",
    "    'spotify:track:3LTMnFa0hhwisyq6ILahyj',\n",
    "]\n",
    "\n",
    "for track_uri in second_playlist:\n",
    "    track_name = track_uri_to_track_name[track_uri]\n",
    "    artist_name = track_uri_to_artist_name[track_uri]\n",
    "    album_name = track_uri_to_album_name[track_uri]\n",
    "    print(f\"{track_name} by {artist_name} from the album {album_name}\")\n",
    "\n",
    "encoded_playlist = encode(second_playlist)\n",
    "context = torch.tensor(encoded_playlist, dtype=torch.long, device=device).unsqueeze(0).to(device)\n",
    "generated_playlist = decode(m.generate(context, max_new_tokens=10)[0].tolist())\n",
    "print('===========')\n",
    "print('Generated playlist:')\n",
    "for track_uri in generated_playlist:\n",
    "    track_name = track_uri_to_track_name[track_uri]\n",
    "    artist_name = track_uri_to_artist_name[track_uri]\n",
    "    album_name = track_uri_to_album_name[track_uri]\n",
    "    print(f\"{track_name} by {artist_name} from the album {album_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Real Slim Shady by Eminem from the album The Marshall Mathers LP\n",
      "Hypnotize - 2014 Remastered Version by The Notorious B.I.G. from the album Life After Death (Remastered Edition)\n",
      "Timber by Pitbull from the album Global Warming: Meltdown (Deluxe Version)\n",
      "===========\n",
      "Generated playlist:\n",
      "The Real Slim Shady by Eminem from the album The Marshall Mathers LP\n",
      "Hypnotize - 2014 Remastered Version by The Notorious B.I.G. from the album Life After Death (Remastered Edition)\n",
      "Timber by Pitbull from the album Global Warming: Meltdown (Deluxe Version)\n",
      "Dilemma by Nelly from the album Nellyville\n",
      "Heaven - S'N'Y Mix Radio Edit by DJ Sammy from the album Wild - The Ultimate Collection\n",
      "Smile by Vitamin C from the album Smile\n",
      "Leave (Get Out) by JoJo from the album Jo Jo\n",
      "Pon de Replay by Rihanna from the album Music Of The Sun\n",
      "Bleeding Love by Leona Lewis from the album Spirit\n",
      "Maneater by Nelly Furtado from the album Loose\n",
      "There It Go (The Whistle Song) by Juelz Santana from the album What The Game's Been Missing!\n",
      "Promiscuous by Nelly Furtado from the album Loose\n",
      "Smack That - Dirty by Akon from the album Konvicted\n"
     ]
    }
   ],
   "source": [
    "third_playlist = [\n",
    "    'spotify:track:3yfqSUWxFvZELEM4PmlwIR',\n",
    "    'spotify:track:7KwZNVEaqikRSBSpyhXK2j',\n",
    "    'spotify:track:3cHyrEgdyYRjgJKSOiOtcS',\n",
    "]\n",
    "\n",
    "for track_uri in third_playlist:\n",
    "    track_name = track_uri_to_track_name[track_uri]\n",
    "    artist_name = track_uri_to_artist_name[track_uri]\n",
    "    album_name = track_uri_to_album_name[track_uri]\n",
    "    print(f\"{track_name} by {artist_name} from the album {album_name}\")\n",
    "\n",
    "encoded_playlist = encode(third_playlist)\n",
    "context = torch.tensor(encoded_playlist, dtype=torch.long, device=device).unsqueeze(0).to(device)\n",
    "generated_playlist = decode(m.generate(context, max_new_tokens=10)[0].tolist())\n",
    "print('===========')\n",
    "print('Generated playlist:')\n",
    "for track_uri in generated_playlist:\n",
    "    track_name = track_uri_to_track_name[track_uri]\n",
    "    artist_name = track_uri_to_artist_name[track_uri]\n",
    "    album_name = track_uri_to_album_name[track_uri]\n",
    "    print(f\"{track_name} by {artist_name} from the album {album_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[77821, 176937, 33997]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_playlist = encode(first_playlist)\n",
    "encoded_playlist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 77821, 176937,  33997]], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor(encoded_playlist, dtype=torch.long, device=device).unsqueeze(0)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIET MOUNTAIN DEW\n",
      "Born To Die - The Paradise Edition\n",
      "By: Lana Del Rey\n",
      "--------------------------------\n",
      "FRANCIS FOREVER\n",
      "Bury Me At Makeout Creek\n",
      "By: Mitski\n",
      "--------------------------------\n",
      "LOVERS ROCK\n",
      "French Exit\n",
      "By: TV Girl\n",
      "--------------------------------\n",
      "GLOW IN THE DARK\n",
      "Post Script\n",
      "By: Jason Gray\n",
      "--------------------------------\n",
      "MT. OLYMPUS (REPRISE)\n",
      "Cadillactica\n",
      "By: Big K.R.I.T.\n",
      "--------------------------------\n",
      "NEMESIS\n",
      "Oil & Gold\n",
      "By: Shriekback\n",
      "--------------------------------\n",
      "ROAD REGRETS\n",
      "Nice, Nice, Very Nice\n",
      "By: Dan Mangan\n",
      "--------------------------------\n",
      "DREAMS FOR PLANS\n",
      "Oral Fixation Vol. 2\n",
      "By: Shakira\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "generated_playlist = decode(m.generate(context, max_new_tokens=5)[0].tolist())\n",
    "\n",
    "for uri in generated_playlist:\n",
    "    print(f\"{track_uri_to_track_name[uri].upper()}\")\n",
    "    print(f\"{track_uri_to_album_name[uri]}\")\n",
    "    print(f\"By: {track_uri_to_artist_name[uri]}\")\n",
    "    print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model so that I can resume training later\n",
    "torch.save(model.state_dict(), 'model_01.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# every once in a while evaluate the loss on train and val sets\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 14\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# sample a batch of data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m      8\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m----> 9\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     11\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 92\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     90\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice)) \u001b[38;5;66;03m# (T,C)\u001b[39;00m\n\u001b[0;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     94\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x) \u001b[38;5;66;03m# (B,T,vocab_size)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 69\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 69\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 37\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 37\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[1;32mIn[17], line 37\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 37\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 15\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     14\u001b[0m     B,T,C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m---> 15\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x) \u001b[38;5;66;03m# (B,T,C)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# compute attention scores (\"affinities\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "model = BigramLanguageModel()\n",
    "model.load_state_dict(torch.load('model_01.pth'))\n",
    "# set model to device\n",
    "model.to(device)\n",
    "\n",
    "# set the optimizer \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate * 3)\n",
    "\n",
    "for iter in range(1000):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
